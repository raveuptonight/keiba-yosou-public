"""
é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

æ¯é€±ç«æ›œ23:00ã«å®Ÿè¡Œã—ã¦ï¼š
1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§ensemble_modelï¼ˆXGBoost + LightGBMï¼‰ã‚’å†å­¦ç¿’
2. åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆå‹åˆ©/è¤‡å‹ï¼‰+ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
3. æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¯”è¼ƒ
4. æ”¹å–„ãŒã‚ã‚Œã°æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
"""

import logging
import json
import os
import shutil
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Optional
import joblib
import numpy as np
import pandas as pd
from sklearn.isotonic import IsotonicRegression

from src.db.connection import get_db
from src.models.fast_train import FastFeatureExtractor
import psycopg2
import psycopg2.extras

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WeeklyRetrain:
    """é€±æ¬¡å†å­¦ç¿’ã‚¯ãƒ©ã‚¹ï¼ˆensemble_modelç”¨ï¼‰"""

    def __init__(
        self,
        model_dir: str = None,
        backup_dir: str = None
    ):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹: ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨Dockerç’°å¢ƒã®ä¸¡æ–¹ã«å¯¾å¿œ
        if model_dir is None:
            if Path("/app/models").exists():
                model_dir = "/app/models"
            else:
                model_dir = str(Path(__file__).parent.parent.parent / "models")
        if backup_dir is None:
            backup_dir = str(Path(model_dir) / "backup")
        self.model_dir = Path(model_dir)
        self.backup_dir = Path(backup_dir)
        self.current_model_path = self.model_dir / "ensemble_model_latest.pkl"

    def _calc_bin_stats(
        self, predicted: np.ndarray, actual: np.ndarray, calibrated: np.ndarray,
        n_bins: int = 20
    ) -> Dict:
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ“ãƒ³çµ±è¨ˆã‚’è¨ˆç®—"""
        from sklearn.metrics import brier_score_loss

        bin_stats = []
        bin_edges = np.linspace(0, 1, n_bins + 1)

        for i in range(n_bins):
            bin_start = bin_edges[i]
            bin_end = bin_edges[i + 1]

            mask = (predicted >= bin_start) & (predicted < bin_end)
            count = mask.sum()

            if count > 0:
                bin_stats.append({
                    'bin_start': float(bin_start),
                    'bin_end': float(bin_end),
                    'count': int(count),
                    'avg_predicted': float(predicted[mask].mean()),
                    'avg_actual': float(actual[mask].mean()),
                    'calibrated': float(calibrated[mask].mean())
                })

        brier_before = brier_score_loss(actual, predicted)
        brier_after = brier_score_loss(actual, calibrated)
        improvement = (brier_before - brier_after) / brier_before * 100 if brier_before > 0 else 0

        return {
            'total_samples': int(len(predicted)),
            'avg_predicted': float(predicted.mean()),
            'avg_actual': float(actual.mean()),
            'brier_before': float(brier_before),
            'brier_after': float(brier_after),
            'improvement': float(improvement),
            'bin_stats': bin_stats
        }

    def save_calibration_to_db(self, calibration_data: Dict, model_version: str) -> bool:
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’DBã«ä¿å­˜"""
        try:
            db = get_db()
            conn = db.get_connection()
            cur = conn.cursor()

            # æ—¢å­˜ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
            cur.execute("UPDATE model_calibration SET is_active = FALSE WHERE is_active = TRUE")

            # æ–°ã—ã„ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
            cur.execute('''
                INSERT INTO model_calibration (
                    model_version, calibration_data, created_at, is_active
                ) VALUES (%s, %s, %s, TRUE)
            ''', (
                model_version,
                psycopg2.extras.Json(calibration_data),
                datetime.now()
            ))

            conn.commit()
            cur.close()
            conn.close()
            logger.info(f"ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’DBã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆversion: {model_version}ï¼‰")
            return True
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def backup_current_model(self) -> Optional[str]:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        if not self.current_model_path.exists():
            logger.warning("ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None

        self.backup_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.backup_dir / f"ensemble_model_{timestamp}.pkl"

        shutil.copy(self.current_model_path, backup_path)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")

        return str(backup_path)

    def train_new_model(self, years: int = 3) -> Dict:
        """æ–°ã—ã„ensemble_modelã‚’å­¦ç¿’ï¼ˆå›å¸° + åˆ†é¡ + ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        import xgboost as xgb
        import lightgbm as lgb
        import catboost as cb

        logger.info(f"ensemble_modelå­¦ç¿’é–‹å§‹ï¼ˆéå»{years}å¹´ï¼‰")

        db = get_db()
        conn = db.get_connection()

        try:
            extractor = FastFeatureExtractor(conn)

            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            current_year = date.today().year
            all_data = []

            for year in range(current_year - years, current_year + 1):
                logger.info(f"  {year}å¹´ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­...")
                year_data = extractor.extract_year_data(year)
                if year_data is not None and len(year_data) > 0:
                    if isinstance(year_data, pd.DataFrame):
                        all_data.append(year_data)
                    else:
                        all_data.append(pd.DataFrame(year_data))
                    logger.info(f"    {len(year_data)}ä»¶")

            if not all_data:
                logger.error("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãªã—")
                return {'status': 'error', 'message': 'no_training_data'}

            # DataFrameçµåˆ
            df = pd.concat(all_data, ignore_index=True)
            logger.info(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}")

            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ–‡å­—åˆ—å‹ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ï¼‰
            exclude_cols = ['race_code', 'umaban', 'bamei', 'target', 'kakutei_chakujun', 'kettonum']
            # æ•°å€¤å‹ã®ã¿æŠ½å‡º
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            feature_cols = [c for c in numeric_cols if c not in exclude_cols]

            X = df[feature_cols].fillna(0)
            y = df['target']

            # åˆ†é¡ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            y_win = (y == 1).astype(int)      # 1ç€ã‹ã©ã†ã‹
            y_quinella = (y <= 2).astype(int)  # 2ç€ä»¥å†…ã‹ã©ã†ã‹ï¼ˆé€£å¯¾ï¼‰
            y_place = (y <= 3).astype(int)    # 3ç€ä»¥å†…ã‹ã©ã†ã‹ï¼ˆè¤‡å‹ï¼‰

            # ===== 3åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—é †ï¼‰=====
            # train (70%): ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨
            # calib (15%): ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚¿ãƒ¼å­¦ç¿’ç”¨ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
            # test  (15%): æœ€çµ‚è©•ä¾¡ç”¨
            n = len(df)
            train_end = int(n * 0.70)
            calib_end = int(n * 0.85)

            X_train = X[:train_end]
            X_calib = X[train_end:calib_end]  # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
            X_test = X[calib_end:]             # æœ€çµ‚è©•ä¾¡ç”¨
            X_val = X_calib  # early stoppingã«ã¯calibãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨

            y_train = y[:train_end]
            y_calib = y[train_end:calib_end]
            y_test = y[calib_end:]
            y_val = y_calib

            y_win_train = y_win[:train_end]
            y_win_calib = y_win[train_end:calib_end]
            y_win_test = y_win[calib_end:]
            y_win_val = y_win_calib

            y_quinella_train = y_quinella[:train_end]
            y_quinella_calib = y_quinella[train_end:calib_end]
            y_quinella_test = y_quinella[calib_end:]
            y_quinella_val = y_quinella_calib

            y_place_train = y_place[:train_end]
            y_place_calib = y_place[train_end:calib_end]
            y_place_test = y_place[calib_end:]
            y_place_val = y_place_calib

            logger.info(f"è¨“ç·´: {len(X_train)}, ã‚­ãƒ£ãƒªãƒ–: {len(X_calib)}, ãƒ†ã‚¹ãƒˆ: {len(X_test)}")

            # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            base_params = {
                'n_estimators': 500,
                'max_depth': 7,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
                'n_jobs': -1
            }

            # CatBoostç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            cb_params = {
                'iterations': 500,
                'depth': 7,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'random_seed': 42,
                'verbose': False,
                'thread_count': -1
            }

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ (XGB:LGB:CB = 30:40:30)
            XGB_WEIGHT = 0.30
            LGB_WEIGHT = 0.40
            CB_WEIGHT = 0.30

            models = {}

            # ===== 1. å›å¸°ãƒ¢ãƒ‡ãƒ« =====
            # XGBoostå›å¸°
            logger.info("XGBoostå›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            xgb_reg = xgb.XGBRegressor(**base_params)
            xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            models['xgb_regressor'] = xgb_reg

            # LightGBMå›å¸°
            logger.info("LightGBMå›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            lgb_reg = lgb.LGBMRegressor(**base_params, verbose=-1)
            lgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)])
            models['lgb_regressor'] = lgb_reg

            # CatBoostå›å¸°
            logger.info("CatBoostå›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            cb_reg = cb.CatBoostRegressor(**cb_params)
            cb_reg.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)
            models['cb_regressor'] = cb_reg

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡
            xgb_pred = xgb_reg.predict(X_val)
            lgb_pred = lgb_reg.predict(X_val)
            cb_pred = cb_reg.predict(X_val)
            ensemble_pred = xgb_pred * XGB_WEIGHT + lgb_pred * LGB_WEIGHT + cb_pred * CB_WEIGHT
            rmse = np.sqrt(np.mean((ensemble_pred - y_val) ** 2))
            logger.info(f"å›å¸°RMSE (3ãƒ¢ãƒ‡ãƒ«ensemble): {rmse:.4f}")

            # ===== 2. å‹åˆ©åˆ†é¡ãƒ¢ãƒ‡ãƒ« =====
            win_weight = len(y_win_train[y_win_train == 0]) / max(len(y_win_train[y_win_train == 1]), 1)

            logger.info("XGBoostå‹åˆ©åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            xgb_win = xgb.XGBClassifier(**base_params, scale_pos_weight=win_weight)
            xgb_win.fit(X_train, y_win_train, eval_set=[(X_val, y_win_val)], verbose=False)
            models['xgb_win'] = xgb_win

            logger.info("LightGBMå‹åˆ©åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            lgb_win = lgb.LGBMClassifier(**base_params, scale_pos_weight=win_weight, verbose=-1)
            lgb_win.fit(X_train, y_win_train, eval_set=[(X_val, y_win_val)])
            models['lgb_win'] = lgb_win

            logger.info("CatBoostå‹åˆ©åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            cb_win = cb.CatBoostClassifier(**cb_params, scale_pos_weight=win_weight)
            cb_win.fit(X_train, y_win_train, eval_set=(X_val, y_win_val), early_stopping_rounds=50)
            models['cb_win'] = cb_win

            # å‹åˆ©ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡
            xgb_win_prob = xgb_win.predict_proba(X_val)[:, 1]
            lgb_win_prob = lgb_win.predict_proba(X_val)[:, 1]
            cb_win_prob = cb_win.predict_proba(X_val)[:, 1]
            ensemble_win_prob = xgb_win_prob * XGB_WEIGHT + lgb_win_prob * LGB_WEIGHT + cb_win_prob * CB_WEIGHT
            win_accuracy = ((ensemble_win_prob > 0.5) == y_win_val).mean()
            logger.info(f"å‹åˆ©åˆ†é¡ç²¾åº¦ (3ãƒ¢ãƒ‡ãƒ«ensemble): {win_accuracy:.4f}")

            # ===== 3. é€£å¯¾åˆ†é¡ãƒ¢ãƒ‡ãƒ« =====
            quinella_weight = len(y_quinella_train[y_quinella_train == 0]) / max(len(y_quinella_train[y_quinella_train == 1]), 1)

            logger.info("XGBoosté€£å¯¾åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            xgb_quinella = xgb.XGBClassifier(**base_params, scale_pos_weight=quinella_weight)
            xgb_quinella.fit(X_train, y_quinella_train, eval_set=[(X_val, y_quinella_val)], verbose=False)
            models['xgb_quinella'] = xgb_quinella

            logger.info("LightGBMé€£å¯¾åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            lgb_quinella = lgb.LGBMClassifier(**base_params, scale_pos_weight=quinella_weight, verbose=-1)
            lgb_quinella.fit(X_train, y_quinella_train, eval_set=[(X_val, y_quinella_val)])
            models['lgb_quinella'] = lgb_quinella

            logger.info("CatBoosté€£å¯¾åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            cb_quinella = cb.CatBoostClassifier(**cb_params, scale_pos_weight=quinella_weight)
            cb_quinella.fit(X_train, y_quinella_train, eval_set=(X_val, y_quinella_val), early_stopping_rounds=50)
            models['cb_quinella'] = cb_quinella

            # é€£å¯¾ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡
            xgb_quinella_prob = xgb_quinella.predict_proba(X_val)[:, 1]
            lgb_quinella_prob = lgb_quinella.predict_proba(X_val)[:, 1]
            cb_quinella_prob = cb_quinella.predict_proba(X_val)[:, 1]
            ensemble_quinella_prob = xgb_quinella_prob * XGB_WEIGHT + lgb_quinella_prob * LGB_WEIGHT + cb_quinella_prob * CB_WEIGHT
            quinella_accuracy = ((ensemble_quinella_prob > 0.5) == y_quinella_val).mean()
            logger.info(f"é€£å¯¾åˆ†é¡ç²¾åº¦ (3ãƒ¢ãƒ‡ãƒ«ensemble): {quinella_accuracy:.4f}")

            # ===== 4. è¤‡å‹åˆ†é¡ãƒ¢ãƒ‡ãƒ« =====
            place_weight = len(y_place_train[y_place_train == 0]) / max(len(y_place_train[y_place_train == 1]), 1)

            logger.info("XGBoostè¤‡å‹åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            xgb_place = xgb.XGBClassifier(**base_params, scale_pos_weight=place_weight)
            xgb_place.fit(X_train, y_place_train, eval_set=[(X_val, y_place_val)], verbose=False)
            models['xgb_place'] = xgb_place

            logger.info("LightGBMè¤‡å‹åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            lgb_place = lgb.LGBMClassifier(**base_params, scale_pos_weight=place_weight, verbose=-1)
            lgb_place.fit(X_train, y_place_train, eval_set=[(X_val, y_place_val)])
            models['lgb_place'] = lgb_place

            logger.info("CatBoostè¤‡å‹åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
            cb_place = cb.CatBoostClassifier(**cb_params, scale_pos_weight=place_weight)
            cb_place.fit(X_train, y_place_train, eval_set=(X_val, y_place_val), early_stopping_rounds=50)
            models['cb_place'] = cb_place

            # è¤‡å‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡
            xgb_place_prob = xgb_place.predict_proba(X_val)[:, 1]
            lgb_place_prob = lgb_place.predict_proba(X_val)[:, 1]
            cb_place_prob = cb_place.predict_proba(X_val)[:, 1]
            ensemble_place_prob = xgb_place_prob * XGB_WEIGHT + lgb_place_prob * LGB_WEIGHT + cb_place_prob * CB_WEIGHT
            place_accuracy = ((ensemble_place_prob > 0.5) == y_place_val).mean()
            logger.info(f"è¤‡å‹åˆ†é¡ç²¾åº¦ (3ãƒ¢ãƒ‡ãƒ«ensemble): {place_accuracy:.4f}")

            # ===== 5. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆcalibãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰=====
            logger.info("ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’ä¸­ï¼ˆcalibãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰...")

            # calibãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚¿ãƒ¼å­¦ç¿’ç”¨ï¼‰
            xgb_win_prob_calib = xgb_win.predict_proba(X_calib)[:, 1]
            lgb_win_prob_calib = lgb_win.predict_proba(X_calib)[:, 1]
            cb_win_prob_calib = cb_win.predict_proba(X_calib)[:, 1]
            ensemble_win_prob_calib = xgb_win_prob_calib * XGB_WEIGHT + lgb_win_prob_calib * LGB_WEIGHT + cb_win_prob_calib * CB_WEIGHT

            xgb_quinella_prob_calib = xgb_quinella.predict_proba(X_calib)[:, 1]
            lgb_quinella_prob_calib = lgb_quinella.predict_proba(X_calib)[:, 1]
            cb_quinella_prob_calib = cb_quinella.predict_proba(X_calib)[:, 1]
            ensemble_quinella_prob_calib = xgb_quinella_prob_calib * XGB_WEIGHT + lgb_quinella_prob_calib * LGB_WEIGHT + cb_quinella_prob_calib * CB_WEIGHT

            xgb_place_prob_calib = xgb_place.predict_proba(X_calib)[:, 1]
            lgb_place_prob_calib = lgb_place.predict_proba(X_calib)[:, 1]
            cb_place_prob_calib = cb_place.predict_proba(X_calib)[:, 1]
            ensemble_place_prob_calib = xgb_place_prob_calib * XGB_WEIGHT + lgb_place_prob_calib * LGB_WEIGHT + cb_place_prob_calib * CB_WEIGHT

            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚¿ãƒ¼å­¦ç¿’
            win_calibrator = IsotonicRegression(out_of_bounds='clip')
            win_calibrator.fit(ensemble_win_prob_calib, y_win_calib)
            models['win_calibrator'] = win_calibrator

            quinella_calibrator = IsotonicRegression(out_of_bounds='clip')
            quinella_calibrator.fit(ensemble_quinella_prob_calib, y_quinella_calib)
            models['quinella_calibrator'] = quinella_calibrator

            place_calibrator = IsotonicRegression(out_of_bounds='clip')
            place_calibrator.fit(ensemble_place_prob_calib, y_place_calib)
            models['place_calibrator'] = place_calibrator

            # ===== 6. æœ€çµ‚è©•ä¾¡ï¼ˆtestãƒ‡ãƒ¼ã‚¿ï¼‰=====
            logger.info("æœ€çµ‚è©•ä¾¡ä¸­ï¼ˆtestãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰...")
            from sklearn.metrics import roc_auc_score, brier_score_loss

            # testãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆ3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
            xgb_pred_test = xgb_reg.predict(X_test)
            lgb_pred_test = lgb_reg.predict(X_test)
            cb_pred_test = cb_reg.predict(X_test)
            ensemble_pred_test = xgb_pred_test * XGB_WEIGHT + lgb_pred_test * LGB_WEIGHT + cb_pred_test * CB_WEIGHT

            xgb_win_prob_test = xgb_win.predict_proba(X_test)[:, 1]
            lgb_win_prob_test = lgb_win.predict_proba(X_test)[:, 1]
            cb_win_prob_test = cb_win.predict_proba(X_test)[:, 1]
            ensemble_win_prob_test = xgb_win_prob_test * XGB_WEIGHT + lgb_win_prob_test * LGB_WEIGHT + cb_win_prob_test * CB_WEIGHT

            xgb_quinella_prob_test = xgb_quinella.predict_proba(X_test)[:, 1]
            lgb_quinella_prob_test = lgb_quinella.predict_proba(X_test)[:, 1]
            cb_quinella_prob_test = cb_quinella.predict_proba(X_test)[:, 1]
            ensemble_quinella_prob_test = xgb_quinella_prob_test * XGB_WEIGHT + lgb_quinella_prob_test * LGB_WEIGHT + cb_quinella_prob_test * CB_WEIGHT

            xgb_place_prob_test = xgb_place.predict_proba(X_test)[:, 1]
            lgb_place_prob_test = lgb_place.predict_proba(X_test)[:, 1]
            cb_place_prob_test = cb_place.predict_proba(X_test)[:, 1]
            ensemble_place_prob_test = xgb_place_prob_test * XGB_WEIGHT + lgb_place_prob_test * LGB_WEIGHT + cb_place_prob_test * CB_WEIGHT

            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
            calibrated_win_test = win_calibrator.predict(ensemble_win_prob_test)
            calibrated_quinella_test = quinella_calibrator.predict(ensemble_quinella_prob_test)
            calibrated_place_test = place_calibrator.predict(ensemble_place_prob_test)

            # AUC-ROCï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‰å¾Œï¼‰
            win_auc_raw = roc_auc_score(y_win_test, ensemble_win_prob_test)
            win_auc = roc_auc_score(y_win_test, calibrated_win_test)
            quinella_auc_raw = roc_auc_score(y_quinella_test, ensemble_quinella_prob_test)
            quinella_auc = roc_auc_score(y_quinella_test, calibrated_quinella_test)
            place_auc_raw = roc_auc_score(y_place_test, ensemble_place_prob_test)
            place_auc = roc_auc_score(y_place_test, calibrated_place_test)

            # Brier Scoreï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‰å¾Œï¼‰
            win_brier_raw = brier_score_loss(y_win_test, ensemble_win_prob_test)
            win_brier = brier_score_loss(y_win_test, calibrated_win_test)
            quinella_brier_raw = brier_score_loss(y_quinella_test, ensemble_quinella_prob_test)
            quinella_brier = brier_score_loss(y_quinella_test, calibrated_quinella_test)
            place_brier_raw = brier_score_loss(y_place_test, ensemble_place_prob_test)
            place_brier = brier_score_loss(y_place_test, calibrated_place_test)

            # Top-3ã‚«ãƒãƒ¼ç‡ï¼ˆtestãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼‰
            top3_coverage = 0.0
            if 'race_code' in df.columns:
                test_df = df.iloc[calib_end:].copy()
                test_df['pred_score'] = ensemble_pred_test
                test_df['win_prob'] = calibrated_win_test

                top3_hits = 0
                total_races = 0
                for race_code, group in test_df.groupby('race_code'):
                    if len(group) < 3:
                        continue
                    winner = group[group['target'] == 1]
                    if len(winner) == 0:
                        continue
                    sorted_group = group.sort_values('pred_score')
                    top3_horses = sorted_group.head(3).index.tolist()
                    if winner.index[0] in top3_horses:
                        top3_hits += 1
                    total_races += 1

                top3_coverage = top3_hits / total_races if total_races > 0 else 0
            else:
                logger.warning("race_codeã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚Top-3ã‚«ãƒãƒ¼ç‡ã‚’ã‚¹ã‚­ãƒƒãƒ—")

            # è©•ä¾¡çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info("=" * 50)
            logger.info("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™ï¼ˆtestãƒ‡ãƒ¼ã‚¿ï¼‰")
            logger.info("=" * 50)
            logger.info(f"å‹åˆ©AUC:      {win_auc:.4f} (raw: {win_auc_raw:.4f})  {'âœ… è‰¯å¥½' if win_auc >= 0.70 else 'âš ï¸ è¦æ”¹å–„'}")
            logger.info(f"é€£å¯¾AUC:      {quinella_auc:.4f} (raw: {quinella_auc_raw:.4f})  {'âœ… è‰¯å¥½' if quinella_auc >= 0.68 else 'âš ï¸ è¦æ”¹å–„'}")
            logger.info(f"è¤‡å‹AUC:      {place_auc:.4f} (raw: {place_auc_raw:.4f})  {'âœ… è‰¯å¥½' if place_auc >= 0.65 else 'âš ï¸ è¦æ”¹å–„'}")
            logger.info(f"å‹åˆ©Brier:    {win_brier:.4f} (raw: {win_brier_raw:.4f}, æ”¹å–„: {(win_brier_raw - win_brier) / win_brier_raw * 100:.1f}%)")
            logger.info(f"é€£å¯¾Brier:    {quinella_brier:.4f} (raw: {quinella_brier_raw:.4f}, æ”¹å–„: {(quinella_brier_raw - quinella_brier) / quinella_brier_raw * 100:.1f}%)")
            logger.info(f"è¤‡å‹Brier:    {place_brier:.4f} (raw: {place_brier_raw:.4f}, æ”¹å–„: {(place_brier_raw - place_brier) / place_brier_raw * 100:.1f}%)")
            logger.info(f"Top-3ã‚«ãƒãƒ¼ç‡: {top3_coverage*100:.1f}%  {'âœ… è‰¯å¥½' if top3_coverage >= 0.55 else 'âš ï¸ è¦æ”¹å–„'}")
            logger.info(f"ã‚­ãƒ£ãƒªãƒ–å¾Œ - å‹ç‡å¹³å‡: {calibrated_win_test.mean():.4f}, é€£å¯¾ç‡å¹³å‡: {calibrated_quinella_test.mean():.4f}, è¤‡å‹ç‡å¹³å‡: {calibrated_place_test.mean():.4f}")
            logger.info("=" * 50)

            # ===== 7. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’DBã«ä¿å­˜ =====
            logger.info("ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è¨ˆç®—ä¸­...")
            calibration_stats = {
                'created_at': datetime.now().isoformat(),
                'win_stats': self._calc_bin_stats(
                    ensemble_win_prob_test, y_win_test.values, calibrated_win_test
                ),
                'quinella_stats': self._calc_bin_stats(
                    ensemble_quinella_prob_test, y_quinella_test.values, calibrated_quinella_test
                ),
                'place_stats': self._calc_bin_stats(
                    ensemble_place_prob_test, y_place_test.values, calibrated_place_test
                )
            }

            model_version = f"v4_quinella_ensemble_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.save_calibration_to_db(calibration_stats, model_version)

            # ä¸€æ™‚ä¿å­˜
            temp_model_path = self.model_dir / "ensemble_model_new.pkl"
            model_data = {
                # å¾Œæ–¹äº’æ›æ€§ï¼ˆæ—§å½¢å¼ï¼‰
                'xgb_model': xgb_reg,
                'lgb_model': lgb_reg,
                'cb_model': cb_reg,  # CatBoostè¿½åŠ 
                # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ç¾¤
                'models': models,
                'feature_names': feature_cols,
                'trained_at': datetime.now().isoformat(),
                'training_samples': len(df),
                'train_size': len(X_train),
                'calib_size': len(X_calib),
                'test_size': len(X_test),
                'validation_rmse': float(rmse),
                'win_accuracy': float(win_accuracy),
                'quinella_accuracy': float(quinella_accuracy),
                'place_accuracy': float(place_accuracy),
                # è©•ä¾¡æŒ‡æ¨™ï¼ˆtestãƒ‡ãƒ¼ã‚¿ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œï¼‰
                'win_auc': float(win_auc),
                'quinella_auc': float(quinella_auc),
                'place_auc': float(place_auc),
                'win_brier': float(win_brier),
                'quinella_brier': float(quinella_brier),
                'place_brier': float(place_brier),
                'top3_coverage': float(top3_coverage),
                'years': years,
                # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿
                'ensemble_weights': {
                    'xgb': XGB_WEIGHT,
                    'lgb': LGB_WEIGHT,
                    'cb': CB_WEIGHT
                },
                'version': 'v5_catboost_ensemble'  # CatBoostå¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³
            }
            joblib.dump(model_data, temp_model_path)

            return {
                'status': 'success',
                'model_path': str(temp_model_path),
                'rmse': float(rmse),
                'win_accuracy': float(win_accuracy),
                'quinella_accuracy': float(quinella_accuracy),
                'place_accuracy': float(place_accuracy),
                'win_auc': float(win_auc),
                'quinella_auc': float(quinella_auc),
                'place_auc': float(place_auc),
                'win_brier': float(win_brier),
                'quinella_brier': float(quinella_brier),
                'top3_coverage': float(top3_coverage),
                'samples': len(df)
            }

        except Exception as e:
            logger.error(f"å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {'status': 'error', 'message': str(e)}

        finally:
            conn.close()

    def compare_models(self, new_model_path: str, test_year: int = 2022) -> Dict:
        """
        æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã‚’ç·åˆè©•ä¾¡ã§æ¯”è¼ƒ

        è©•ä¾¡æŒ‡æ¨™:
        - AUC (å‹åˆ©/é€£å¯¾/è¤‡å‹)
        - Top-3ã‚«ãƒãƒ¼ç‡
        - å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå˜å‹ãƒ»è¤‡å‹ï¼‰
        - RMSE

        ç·åˆã‚¹ã‚³ã‚¢ã§åˆ¤æ–­ï¼ˆAUCé‡è¦–ã€å›åç‡ã‚‚è€ƒæ…®ï¼‰

        æ³¨æ„: test_yearã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„å¹´ã‚’æŒ‡å®šã™ã‚‹ã“ã¨
              ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2022å¹´ï¼ˆå­¦ç¿’ã¯2023å¹´ä»¥é™ã‚’ä½¿ç”¨ï¼‰
        """
        from sklearn.metrics import roc_auc_score

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ç·åˆæ¯”è¼ƒï¼ˆãƒ†ã‚¹ãƒˆå¹´: {test_year}ï¼‰")
        logger.info(f"â€»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¤–ã®å¹´ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        try:
            old_model_data = joblib.load(self.current_model_path)
            new_model_data = joblib.load(new_model_path)
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {'status': 'error', 'message': str(e)}

        old_features = old_model_data['feature_names']
        new_features = new_model_data['feature_names']

        # åˆ†é¡ãƒ¢ãƒ‡ãƒ«å–å¾—
        old_models = old_model_data.get('models', {})
        new_models = new_model_data.get('models', {})

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ‰•æˆ»ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚€ï¼‰
        db = get_db()
        conn = db.get_connection()

        try:
            extractor = FastFeatureExtractor(conn)
            test_data = extractor.extract_year_data(test_year)

            if test_data is None or len(test_data) == 0:
                return {'status': 'error', 'message': 'no_test_data'}

            if isinstance(test_data, list):
                df = pd.DataFrame(test_data)
            else:
                df = test_data

            logger.info(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}")

            # æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            payouts = self._get_payouts_for_year(conn, test_year)

            # ä¸¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ
            # ç‰¹å¾´é‡ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°ã—ã„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§æ—§ãƒ¢ãƒ‡ãƒ«ã¯è©•ä¾¡ã§ããªã„å ´åˆãŒã‚ã‚‹ï¼‰
            missing_old_features = set(old_features) - set(df.columns)
            if missing_old_features:
                logger.warning(f"âš ï¸ æ—§ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡ãŒä¸è¶³: {missing_old_features}")
                logger.info("ç‰¹å¾´é‡æ§‹æˆãŒå¤‰æ›´ã•ã‚ŒãŸãŸã‚ã€æ–°ãƒ¢ãƒ‡ãƒ«ã®ã¿è©•ä¾¡ã—ã¾ã™")
                old_eval = None
            else:
                old_eval = self._evaluate_model(df, old_model_data, old_features, payouts, "æ—§ãƒ¢ãƒ‡ãƒ«")
            new_eval = self._evaluate_model(df, new_model_data, new_features, payouts, "æ–°ãƒ¢ãƒ‡ãƒ«")

            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            new_score = self._calculate_composite_score(new_eval)

            logger.info("=" * 50)
            logger.info("ğŸ“Š ç·åˆè©•ä¾¡çµæœ")
            logger.info("=" * 50)

            if old_eval is not None:
                old_score = self._calculate_composite_score(old_eval)
                improvement = new_score - old_score
                logger.info(f"æ—§ãƒ¢ãƒ‡ãƒ«ç·åˆã‚¹ã‚³ã‚¢: {old_score:.4f}")
                logger.info(f"æ–°ãƒ¢ãƒ‡ãƒ«ç·åˆã‚¹ã‚³ã‚¢: {new_score:.4f}")
                logger.info(f"æ”¹å–„: {improvement:+.4f} ({'âœ… æ–°ãƒ¢ãƒ‡ãƒ«å„ªä½' if improvement > 0 else 'âŒ æ—§ãƒ¢ãƒ‡ãƒ«ç¶­æŒ'})")
            else:
                # ç‰¹å¾´é‡å¤‰æ›´æ™‚ã¯æ—§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãªã—ã€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ¡ç”¨
                old_score = 0.0
                improvement = 1.0  # å¼·åˆ¶çš„ã«æ–°ãƒ¢ãƒ‡ãƒ«æ¡ç”¨
                logger.info("æ—§ãƒ¢ãƒ‡ãƒ«: è©•ä¾¡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç‰¹å¾´é‡å¤‰æ›´ï¼‰")
                logger.info(f"æ–°ãƒ¢ãƒ‡ãƒ«ç·åˆã‚¹ã‚³ã‚¢: {new_score:.4f}")
                logger.info("âœ… ç‰¹å¾´é‡æ§‹æˆå¤‰æ›´ã®ãŸã‚æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨")

            return {
                'status': 'success',
                'old_eval': old_eval,
                'new_eval': new_eval,
                'old_score': float(old_score),
                'new_score': float(new_score),
                'improvement': float(improvement),
                'test_samples': len(df),
                # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚RMSEã‚‚å«ã‚ã‚‹
                'old_rmse': old_eval.get('rmse', 0) if old_eval else 0,
                'new_rmse': new_eval.get('rmse', 0)
            }

        finally:
            conn.close()

    def _evaluate_model(self, df: pd.DataFrame, model_data: Dict, features: list,
                        payouts: Dict, model_name: str) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«ã®ç·åˆè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        from sklearn.metrics import roc_auc_score

        models = model_data.get('models', {})
        weights = model_data.get('ensemble_weights')  # CatBoostå¯¾å¿œã®é‡ã¿
        X = df[features].fillna(0)

        # å›å¸°äºˆæ¸¬ï¼ˆç€é †ï¼‰
        xgb_reg = models.get('xgb_regressor') or model_data.get('xgb_model')
        lgb_reg = models.get('lgb_regressor') or model_data.get('lgb_model')
        cb_reg = models.get('cb_regressor') or model_data.get('cb_model')

        if cb_reg is not None and weights is not None:
            # 3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            reg_pred = (xgb_reg.predict(X) * weights['xgb'] +
                       lgb_reg.predict(X) * weights['lgb'] +
                       cb_reg.predict(X) * weights['cb'])
        else:
            # 2ãƒ¢ãƒ‡ãƒ«å¹³å‡ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            reg_pred = (xgb_reg.predict(X) + lgb_reg.predict(X)) / 2

        rmse = float(np.sqrt(np.mean((reg_pred - df['target']) ** 2)))

        # åˆ†é¡äºˆæ¸¬ï¼ˆå‹åˆ©/é€£å¯¾/è¤‡å‹ï¼‰
        eval_result = {'rmse': rmse}

        # CatBooståˆ†é¡ãƒ¢ãƒ‡ãƒ«
        cb_win = models.get('cb_win')
        cb_quinella = models.get('cb_quinella')
        cb_place = models.get('cb_place')

        # å‹åˆ©AUC
        if 'xgb_win' in models and 'lgb_win' in models:
            win_prob = self._get_ensemble_proba(models['xgb_win'], models['lgb_win'], X,
                                                 models.get('win_calibrator'),
                                                 cb_clf=cb_win, weights=weights)
            win_actual = (df['target'] == 1).astype(int)
            try:
                eval_result['win_auc'] = float(roc_auc_score(win_actual, win_prob))
            except:
                eval_result['win_auc'] = 0.5

        # é€£å¯¾AUC
        if 'xgb_quinella' in models and 'lgb_quinella' in models:
            quinella_prob = self._get_ensemble_proba(models['xgb_quinella'], models['lgb_quinella'], X,
                                                      models.get('quinella_calibrator'),
                                                      cb_clf=cb_quinella, weights=weights)
            quinella_actual = (df['target'] <= 2).astype(int)
            try:
                eval_result['quinella_auc'] = float(roc_auc_score(quinella_actual, quinella_prob))
            except:
                eval_result['quinella_auc'] = 0.5

        # è¤‡å‹AUC
        if 'xgb_place' in models and 'lgb_place' in models:
            place_prob = self._get_ensemble_proba(models['xgb_place'], models['lgb_place'], X,
                                                   models.get('place_calibrator'),
                                                   cb_clf=cb_place, weights=weights)
            place_actual = (df['target'] <= 3).astype(int)
            try:
                eval_result['place_auc'] = float(roc_auc_score(place_actual, place_prob))
            except:
                eval_result['place_auc'] = 0.5

        # Top-3ã‚«ãƒãƒ¼ç‡ï¼ˆãƒ¬ãƒ¼ã‚¹ã”ã¨ã«è¨ˆç®—ï¼‰
        df_eval = df.copy()
        df_eval['pred_rank'] = reg_pred

        if 'race_code' in df_eval.columns:
            top3_hits = 0
            total_races = 0

            for race_code, race_df in df_eval.groupby('race_code'):
                race_df_sorted = race_df.sort_values('pred_rank')
                top3_pred = race_df_sorted.head(3)['target'].values
                if any(t == 1 for t in top3_pred):
                    top3_hits += 1
                total_races += 1

            eval_result['top3_coverage'] = float(top3_hits / total_races) if total_races > 0 else 0

        # å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        returns = self._simulate_returns(df_eval, reg_pred, payouts)
        eval_result.update(returns)

        # ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ã€{model_name}ã€‘")
        logger.info(f"  RMSE: {rmse:.4f}")
        logger.info(f"  å‹åˆ©AUC: {eval_result.get('win_auc', 0):.4f}")
        logger.info(f"  é€£å¯¾AUC: {eval_result.get('quinella_auc', 0):.4f}")
        logger.info(f"  è¤‡å‹AUC: {eval_result.get('place_auc', 0):.4f}")
        logger.info(f"  Top-3ã‚«ãƒãƒ¼ç‡: {eval_result.get('top3_coverage', 0)*100:.1f}%")
        logger.info(f"  å˜å‹å›åç‡: {eval_result.get('tansho_return', 0)*100:.1f}%")
        logger.info(f"  è¤‡å‹å›åç‡: {eval_result.get('fukusho_return', 0)*100:.1f}%")

        return eval_result

    def _get_ensemble_proba(self, xgb_clf, lgb_clf, X, calibrator=None, cb_clf=None, weights=None) -> np.ndarray:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨ã€CatBoostå¯¾å¿œï¼‰"""
        xgb_prob = xgb_clf.predict_proba(X)[:, 1]
        lgb_prob = lgb_clf.predict_proba(X)[:, 1]

        if cb_clf is not None and weights is not None:
            # 3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            cb_prob = cb_clf.predict_proba(X)[:, 1]
            raw_prob = xgb_prob * weights['xgb'] + lgb_prob * weights['lgb'] + cb_prob * weights['cb']
        elif weights is not None:
            # 2ãƒ¢ãƒ‡ãƒ«é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            raw_prob = xgb_prob * weights.get('xgb', 0.5) + lgb_prob * weights.get('lgb', 0.5)
        else:
            # 2ãƒ¢ãƒ‡ãƒ«å˜ç´”å¹³å‡ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            raw_prob = (xgb_prob + lgb_prob) / 2

        if calibrator is not None:
            try:
                return calibrator.predict(raw_prob)
            except:
                pass
        return raw_prob

    def _get_payouts_for_year(self, conn, year: int) -> Dict:
        """å¹´é–“ã®æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            cur.execute('''
                SELECT race_code,
                       tansho1_umaban, tansho1_haraimodoshikin,
                       fukusho1_umaban, fukusho1_haraimodoshikin,
                       fukusho2_umaban, fukusho2_haraimodoshikin,
                       fukusho3_umaban, fukusho3_haraimodoshikin
                FROM haraimodoshi
                WHERE EXTRACT(YEAR FROM TO_DATE(SUBSTRING(race_code, 1, 8), 'YYYYMMDD')) = %s
            ''', (year,))

            payouts = {}
            for row in cur.fetchall():
                race_code = row['race_code']

                # å˜å‹æ‰•æˆ»é‡‘ã‚’å®‰å…¨ã«å–å¾—
                tansho_umaban = row['tansho1_umaban']
                tansho_payout = row['tansho1_haraimodoshikin']
                tansho_umaban_str = str(tansho_umaban).strip() if tansho_umaban else None
                try:
                    tansho_payout_int = int(str(tansho_payout).strip()) if tansho_payout and str(tansho_payout).strip() else 0
                except ValueError:
                    tansho_payout_int = 0

                payouts[race_code] = {
                    'tansho': {
                        'umaban': tansho_umaban_str,
                        'payout': tansho_payout_int
                    },
                    'fukusho': []
                }

                # è¤‡å‹æ‰•æˆ»é‡‘ã‚’å®‰å…¨ã«å–å¾—
                for i in range(1, 4):
                    umaban = row.get(f'fukusho{i}_umaban')
                    payout = row.get(f'fukusho{i}_haraimodoshikin')
                    if umaban and str(umaban).strip():
                        try:
                            payout_int = int(str(payout).strip()) if payout and str(payout).strip() else 0
                        except ValueError:
                            payout_int = 0
                        if payout_int > 0:
                            payouts[race_code]['fukusho'].append({
                                'umaban': str(umaban).strip(),
                                'payout': payout_int
                            })
            cur.close()
            logger.info(f"æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(payouts)}ãƒ¬ãƒ¼ã‚¹")
            return payouts
        except Exception as e:
            logger.warning(f"æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _simulate_returns(self, df: pd.DataFrame, predictions: np.ndarray, payouts: Dict) -> Dict:
        """å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå„ãƒ¬ãƒ¼ã‚¹1ä½äºˆæƒ³ã«100å††è³­ã‘ï¼‰"""
        df_sim = df.copy()
        df_sim['pred_rank'] = predictions

        tansho_bet = 0
        tansho_win = 0
        fukusho_bet = 0
        fukusho_win = 0

        # é¦¬ç•ªã‚«ãƒ©ãƒ ã®ç¢ºèªï¼ˆumaban ã¾ãŸã¯ horse_numberï¼‰
        umaban_col = 'umaban' if 'umaban' in df_sim.columns else 'horse_number'
        if 'race_code' not in df_sim.columns or umaban_col not in df_sim.columns:
            logger.warning(f"å›åç‡è¨ˆç®—ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“: race_code={('race_code' in df_sim.columns)}, {umaban_col}={(umaban_col in df_sim.columns)}")
            return {'tansho_return': 0, 'fukusho_return': 0}

        for race_code, race_df in df_sim.groupby('race_code'):
            race_df_sorted = race_df.sort_values('pred_rank')
            if len(race_df_sorted) == 0:
                continue

            top1 = race_df_sorted.iloc[0]
            pred_umaban = str(int(top1[umaban_col]))

            payout_data = payouts.get(race_code, {})

            # å˜å‹
            tansho_bet += 100
            tansho_info = payout_data.get('tansho', {})
            if tansho_info.get('umaban') == pred_umaban:
                tansho_win += tansho_info.get('payout', 0)

            # è¤‡å‹
            fukusho_bet += 100
            for fuku in payout_data.get('fukusho', []):
                if fuku.get('umaban') == pred_umaban:
                    fukusho_win += fuku.get('payout', 0)
                    break

        return {
            'tansho_return': float(tansho_win / tansho_bet) if tansho_bet > 0 else 0,
            'fukusho_return': float(fukusho_win / fukusho_bet) if fukusho_bet > 0 else 0,
            'tansho_bet': tansho_bet,
            'tansho_win': tansho_win,
            'fukusho_bet': fukusho_bet,
            'fukusho_win': fukusho_win
        }

    def _calculate_composite_score(self, eval_result: Dict) -> float:
        """
        ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—

        é‡ã¿ä»˜ã‘:
        - å‹åˆ©AUC: 25%ï¼ˆåˆ†é¡ç²¾åº¦ã®æ ¸å¿ƒï¼‰
        - é€£å¯¾AUC: 15%
        - è¤‡å‹AUC: 15%
        - Top-3ã‚«ãƒãƒ¼ç‡: 20%ï¼ˆå®Ÿç”¨æ€§ï¼‰
        - å˜å‹å›åç‡: 15%ï¼ˆåç›Šæ€§ï¼‰
        - è¤‡å‹å›åç‡: 10%
        """
        weights = {
            'win_auc': 0.25,
            'quinella_auc': 0.15,
            'place_auc': 0.15,
            'top3_coverage': 0.20,
            'tansho_return': 0.15,
            'fukusho_return': 0.10
        }

        score = 0
        for metric, weight in weights.items():
            value = eval_result.get(metric, 0)
            # AUCã¯0.5-1.0ã®ç¯„å›²ãªã®ã§ã€0.5ã‚’å¼•ã„ã¦2å€ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«
            if 'auc' in metric:
                value = (value - 0.5) * 2  # 0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
            # å›åç‡ã¯1.0ãŒ100%ãªã®ã§ãã®ã¾ã¾
            score += value * weight

        return score

    def deploy_new_model(self, new_model_path: str):
        """æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤"""
        # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        self.backup_current_model()

        # æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã«é…ç½®
        shutil.move(new_model_path, self.current_model_path)
        logger.info(f"æ–°ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†: {self.current_model_path}")

    def send_notification(self, result: Dict):
        """å†å­¦ç¿’çµæœã‚’é€šçŸ¥"""
        webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
        if not webhook_url:
            return

        try:
            import requests

            training = result.get('training', {})

            # è©•ä¾¡æŒ‡æ¨™ã‚’å–å¾—
            win_auc = training.get('win_auc', 0)
            place_auc = training.get('place_auc', 0)
            win_brier = training.get('win_brier', 0)
            top3_coverage = training.get('top3_coverage', 0)

            # è©•ä¾¡ã‚¢ã‚¤ã‚³ãƒ³
            def get_icon(value, good, excellent, lower_is_better=False):
                if lower_is_better:
                    if value <= excellent:
                        return "ğŸŒŸ"
                    elif value <= good:
                        return "âœ…"
                    else:
                        return "âš ï¸"
                else:
                    if value >= excellent:
                        return "ğŸŒŸ"
                    elif value >= good:
                        return "âœ…"
                    else:
                        return "âš ï¸"

            if result.get('deployed'):
                lines = [
                    "ğŸ”„ **é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†**",
                    "",
                    f"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {training.get('samples', 0):,}",
                    "",
                    "ğŸ“Š **è©•ä¾¡æŒ‡æ¨™:**",
                    f"```",
                    f"å‹åˆ©AUC:       {win_auc:.4f} {get_icon(win_auc, 0.70, 0.80)}",
                    f"è¤‡å‹AUC:       {place_auc:.4f} {get_icon(place_auc, 0.65, 0.75)}",
                    f"Brier(å‹åˆ©):   {win_brier:.4f} {get_icon(win_brier, 0.07, 0.05, True)}",
                    f"Top-3ã‚«ãƒãƒ¼ç‡: {top3_coverage*100:.1f}% {get_icon(top3_coverage, 0.55, 0.65)}",
                    f"```",
                    "",
                    "âœ… æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã—ãŸ"
                ]
            else:
                lines = [
                    "ğŸ”„ **é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†**",
                    "",
                    f"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {training.get('samples', 0):,}",
                    "",
                    "ğŸ“Š **è©•ä¾¡æŒ‡æ¨™:**",
                    f"```",
                    f"å‹åˆ©AUC:       {win_auc:.4f} {get_icon(win_auc, 0.70, 0.80)}",
                    f"è¤‡å‹AUC:       {place_auc:.4f} {get_icon(place_auc, 0.65, 0.75)}",
                    f"Brier(å‹åˆ©):   {win_brier:.4f} {get_icon(win_brier, 0.07, 0.05, True)}",
                    f"Top-3ã‚«ãƒãƒ¼ç‡: {top3_coverage*100:.1f}% {get_icon(top3_coverage, 0.55, 0.65)}",
                    f"```",
                    "",
                    "âš ï¸ æ”¹å–„ãªã—ã®ãŸã‚ç¾è¡Œãƒ¢ãƒ‡ãƒ«ã‚’ç¶­æŒ"
                ]

            payload = {"content": "\n".join(lines)}
            requests.post(webhook_url, json=payload, timeout=10)

        except Exception as e:
            logger.error(f"é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def run_weekly_job(self, force_deploy: bool = False, notify: bool = True, years: int = 3):
        """é€±æ¬¡ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ"""
        logger.info("=" * 50)
        logger.info(f"é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’é–‹å§‹: {datetime.now()}")
        logger.info("=" * 50)

        result = {
            'date': date.today().isoformat(),
            'deployed': False
        }

        # 1. æ–°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        training_result = self.train_new_model(years=years)
        result['training'] = training_result

        if training_result['status'] != 'success':
            logger.error(f"å­¦ç¿’å¤±æ•—: {training_result}")
            return result

        new_model_path = training_result['model_path']

        # 2. æ¯”è¼ƒï¼ˆç¾è¡Œãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
        if self.current_model_path.exists():
            comparison = self.compare_models(new_model_path)
            result['comparison'] = comparison

            if comparison['status'] == 'success':
                # æ”¹å–„ãŒã‚ã‚Œã°ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆã¾ãŸã¯å¼·åˆ¶ãƒ‡ãƒ—ãƒ­ã‚¤ï¼‰
                if comparison['improvement'] > 0 or force_deploy:
                    self.deploy_new_model(new_model_path)
                    result['deployed'] = True
                    logger.info("æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã—ãŸ")
                else:
                    logger.info("æ”¹å–„ãªã—ã€ç¾è¡Œãƒ¢ãƒ‡ãƒ«ã‚’ç¶­æŒ")
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    Path(new_model_path).unlink()
            elif force_deploy:
                # æ¯”è¼ƒå¤±æ•—ã§ã‚‚å¼·åˆ¶ãƒ‡ãƒ—ãƒ­ã‚¤
                self.deploy_new_model(new_model_path)
                result['deployed'] = True
                logger.info("å¼·åˆ¶ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã—ãŸï¼ˆæ¯”è¼ƒå¤±æ•—ï¼‰")
        else:
            # ç¾è¡Œãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯ãã®ã¾ã¾ãƒ‡ãƒ—ãƒ­ã‚¤
            shutil.move(new_model_path, self.current_model_path)
            result['deployed'] = True
            result['comparison'] = {'note': 'initial_deployment'}
            logger.info("åˆå›ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†")

        # 3. é€šçŸ¥
        if notify:
            self.send_notification(result)

        # çµæœä¿å­˜
        result_path = self.model_dir / f"retrain_result_{date.today().strftime('%Y%m%d')}.json"
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        return result


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="é€±æ¬¡ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ï¼ˆensemble_modelï¼‰")
    parser.add_argument("--force", "-f", action="store_true", help="æ”¹å–„ãªã—ã§ã‚‚ãƒ‡ãƒ—ãƒ­ã‚¤")
    parser.add_argument("--no-notify", action="store_true", help="Discordé€šçŸ¥ã—ãªã„")
    parser.add_argument("--years", "-y", type=int, default=3, help="å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹å¹´æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3å¹´ï¼‰")

    args = parser.parse_args()

    retrain = WeeklyRetrain()
    result = retrain.run_weekly_job(
        force_deploy=args.force,
        notify=not args.no_notify,
        years=args.years
    )

    print("\n=== å†å­¦ç¿’çµæœ ===")
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
