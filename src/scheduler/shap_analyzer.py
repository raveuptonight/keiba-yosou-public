"""
SHAPåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

é€±æœ«ãƒ¬ãƒ¼ã‚¹ã®äºˆæƒ³çµæœã‚’åˆ†æã—ã€ç‰¹å¾´é‡ã®å¯„ä¸åº¦ã‚’å¯è¦–åŒ–ã™ã‚‹
- çš„ä¸­/å¤–ã‚Œåˆ¥ã®ç‰¹å¾´é‡é‡è¦åº¦æ¯”è¼ƒ
- éå¤§è©•ä¾¡/éå°è©•ä¾¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
- é€±æ¬¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import logging
import json
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import pandas as pd
import joblib

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

from src.db.connection import get_db
from src.models.fast_train import FastFeatureExtractor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ShapAnalyzer:
    """SHAPå€¤ã«ã‚ˆã‚‹äºˆæƒ³åˆ†æ"""

    def __init__(self, model_path: str = "/app/models/ensemble_model_latest.pkl"):
        self.model_path = model_path
        self.xgb_model = None
        self.lgb_model = None
        self.feature_names = None
        self.explainer = None
        self._load_model()

    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            model_data = joblib.load(self.model_path)
            models_dict = model_data.get('models', {})

            # å›å¸°ãƒ¢ãƒ‡ãƒ«å–å¾—
            if 'xgb_regressor' in models_dict:
                self.xgb_model = models_dict['xgb_regressor']
                self.lgb_model = models_dict.get('lgb_regressor')
            elif 'xgb_model' in model_data:
                self.xgb_model = model_data['xgb_model']
                self.lgb_model = model_data.get('lgb_model')
            elif 'xgboost' in models_dict:
                self.xgb_model = models_dict.get('xgboost')
                self.lgb_model = models_dict.get('lightgbm')

            self.feature_names = model_data.get('feature_names', [])
            logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(self.feature_names)}ç‰¹å¾´é‡")

            # SHAP Explainerã‚’åˆæœŸåŒ–ï¼ˆXGBoostã‚’ä½¿ç”¨ï¼‰
            if SHAP_AVAILABLE and self.xgb_model is not None:
                self.explainer = shap.TreeExplainer(self.xgb_model)
                logger.info("SHAP TreeExplaineråˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

    def get_recent_race_dates(self, days_back: int = 7) -> List[date]:
        """ç›´è¿‘ã®ãƒ¬ãƒ¼ã‚¹æ—¥ï¼ˆäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ—¥ï¼‰ã‚’å–å¾—"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            cur.execute('''
                SELECT DISTINCT race_date
                FROM predictions
                WHERE race_date >= %s AND race_date < %s
                ORDER BY race_date DESC
            ''', (date.today() - timedelta(days=days_back), date.today()))

            dates = [row[0] for row in cur.fetchall()]
            cur.close()
            return dates
        finally:
            conn.close()

    def get_predictions_from_db(self, target_date: date) -> List[Dict]:
        """DBã‹ã‚‰äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ã‚¹ã”ã¨ã«æœ€æ–°ã®äºˆæƒ³ã®ã¿ï¼‰"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            cur.execute('''
                SELECT DISTINCT ON (race_id)
                    prediction_id,
                    race_id,
                    race_date,
                    prediction_result,
                    predicted_at
                FROM predictions
                WHERE race_date = %s
                ORDER BY race_id, predicted_at DESC
            ''', (target_date,))

            predictions = []
            for row in cur.fetchall():
                prediction_result = row[3]
                if isinstance(prediction_result, str):
                    try:
                        prediction_result = json.loads(prediction_result)
                    except json.JSONDecodeError:
                        prediction_result = {}

                predictions.append({
                    'prediction_id': row[0],
                    'race_code': row[1],
                    'race_date': row[2],
                    'prediction_result': prediction_result,
                })

            cur.close()
            logger.info(f"äºˆæƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(predictions)}ãƒ¬ãƒ¼ã‚¹ ({target_date})")
            return predictions

        finally:
            conn.close()

    def get_race_results(self, target_date: date) -> Dict[str, Dict]:
        """ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            kaisai_gappi = target_date.strftime("%m%d")
            kaisai_nen = str(target_date.year)

            cur.execute('''
                SELECT race_code, umaban, kakutei_chakujun, bamei
                FROM umagoto_race_joho
                WHERE race_code IN (
                    SELECT DISTINCT race_code
                    FROM race_shosai
                    WHERE kaisai_nen = %s AND kaisai_gappi = %s
                      AND data_kubun IN ('6', '7')
                )
                AND data_kubun IN ('6', '7')
                ORDER BY race_code, kakutei_chakujun::int
            ''', (kaisai_nen, kaisai_gappi))

            results = {}
            for row in cur.fetchall():
                race_code = row[0]
                if race_code not in results:
                    results[race_code] = {'horses': []}
                results[race_code]['horses'].append({
                    'umaban': row[1],
                    'chakujun': int(row[2]) if row[2] else 99,
                    'bamei': row[3],
                })

            cur.close()
            return results

        finally:
            conn.close()

    def extract_features_for_race(self, race_code: str) -> Optional[pd.DataFrame]:
        """ãƒ¬ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        db = get_db()
        conn = db.get_connection()

        try:
            extractor = FastFeatureExtractor(conn)
            cur = conn.cursor()

            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—
            cur.execute('''
                SELECT kaisai_nen, race_code, kaisai_gappi, keibajo_code,
                       kyori, track_code, grade_code,
                       shiba_babajotai_code, dirt_babajotai_code
                FROM race_shosai
                WHERE race_code = %s
            ''', (race_code,))
            race_row = cur.fetchone()
            if not race_row:
                return None

            race_cols = ['kaisai_nen', 'race_code', 'kaisai_gappi', 'keibajo_code',
                        'kyori', 'track_code', 'grade_code',
                        'shiba_babajotai_code', 'dirt_babajotai_code']
            races = [dict(zip(race_cols, race_row))]
            year = int(race_row[0])

            # å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾—
            cur.execute('''
                SELECT
                    race_code, umaban, wakuban, ketto_toroku_bango,
                    seibetsu_code, barei, futan_juryo,
                    blinker_shiyo_kubun, kishu_code, chokyoshi_code,
                    bataiju, zogen_sa, bamei, kakutei_chakujun
                FROM umagoto_race_joho
                WHERE race_code = %s
                  AND data_kubun IN ('6', '7')
                ORDER BY umaban::int
            ''', (race_code,))

            cols = [d[0] for d in cur.description]
            rows = cur.fetchall()
            entries = [dict(zip(cols, row)) for row in rows]

            if not entries:
                return None

            # éå»æˆç¸¾å–å¾—
            kettonums = [e['ketto_toroku_bango'] for e in entries if e.get('ketto_toroku_bango')]
            past_stats = extractor._get_past_stats_batch(kettonums)

            # é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            extractor._cache_jockey_trainer_stats(year)

            # è¿½åŠ ãƒ‡ãƒ¼ã‚¿
            jh_pairs = [(e.get('kishu_code', ''), e.get('ketto_toroku_bango', ''))
                        for e in entries if e.get('kishu_code') and e.get('ketto_toroku_bango')]
            jockey_horse_stats = extractor._get_jockey_horse_combo_batch(jh_pairs)
            surface_stats = extractor._get_surface_stats_batch(kettonums)
            turn_stats = extractor._get_turn_rates_batch(kettonums)
            for kettonum, stats in turn_stats.items():
                if kettonum in past_stats:
                    past_stats[kettonum]['right_turn_rate'] = stats['right_turn_rate']
                    past_stats[kettonum]['left_turn_rate'] = stats['left_turn_rate']
            training_stats = extractor._get_training_stats_batch(kettonums)

            # ç‰¹å¾´é‡ç”Ÿæˆ
            features_list = []
            for entry in entries:
                features = extractor._build_features(
                    entry, races, past_stats,
                    jockey_horse_stats=jockey_horse_stats,
                    distance_stats=surface_stats,
                    training_stats=training_stats
                )
                if features:
                    features['bamei'] = entry.get('bamei', '')
                    features['actual_chakujun'] = int(entry.get('kakutei_chakujun', 99))
                    features_list.append(features)

            cur.close()

            if not features_list:
                return None

            return pd.DataFrame(features_list)

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({race_code}): {e}")
            return None
        finally:
            conn.close()

    def calculate_shap_values(self, X: pd.DataFrame) -> np.ndarray:
        """SHAPå€¤ã‚’è¨ˆç®—"""
        if not SHAP_AVAILABLE:
            logger.warning("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None

        if self.explainer is None:
            logger.warning("SHAP ExplainerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        try:
            shap_values = self.explainer.shap_values(X)
            return shap_values
        except Exception as e:
            logger.error(f"SHAPå€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def analyze_race(self, race_code: str, prediction: Dict) -> Optional[Dict]:
        """1ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æ"""
        # ç‰¹å¾´é‡æŠ½å‡º
        df = self.extract_features_for_race(race_code)
        if df is None or df.empty:
            return None

        # äºˆæƒ³çµæœã‹ã‚‰1ä½äºˆæƒ³é¦¬ã‚’ç‰¹å®š
        ranked_horses = prediction.get('prediction_result', {}).get('ranked_horses', [])
        if not ranked_horses:
            return None

        pred_1st = ranked_horses[0]
        pred_1st_umaban = str(pred_1st.get('horse_number', '')).zfill(2)

        # å®Ÿéš›ã®ç€é †ã‚’å–å¾—
        horse_row = df[df['umaban'].astype(str).str.zfill(2) == pred_1st_umaban]
        if horse_row.empty:
            return None

        actual_chakujun = int(horse_row['actual_chakujun'].iloc[0])

        # ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
        X = df[self.feature_names].fillna(0)

        # SHAPå€¤è¨ˆç®—
        shap_values = self.calculate_shap_values(X)
        if shap_values is None:
            return None

        # 1ä½äºˆæƒ³é¦¬ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        horse_idx = df[df['umaban'].astype(str).str.zfill(2) == pred_1st_umaban].index[0]
        horse_shap = shap_values[horse_idx]

        # ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’ã¾ã¨ã‚ã‚‹
        feature_contributions = {}
        for i, fname in enumerate(self.feature_names):
            feature_contributions[fname] = float(horse_shap[i])

        # çš„ä¸­åˆ¤å®š
        is_hit = actual_chakujun == 1
        is_place = actual_chakujun <= 3

        return {
            'race_code': race_code,
            'pred_1st_umaban': pred_1st_umaban,
            'pred_1st_bamei': pred_1st.get('horse_name', ''),
            'actual_chakujun': actual_chakujun,
            'is_hit': is_hit,
            'is_place': is_place,
            'feature_contributions': feature_contributions,
            'win_prob': pred_1st.get('win_probability', 0),
        }

    def analyze_dates(self, target_dates: List[date]) -> Dict:
        """è¤‡æ•°æ—¥ã®ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æ"""
        all_analyses = []

        for target_date in target_dates:
            predictions = self.get_predictions_from_db(target_date)

            for pred in predictions:
                race_code = pred['race_code']
                analysis = self.analyze_race(race_code, pred)
                if analysis:
                    analysis['date'] = str(target_date)
                    all_analyses.append(analysis)

        if not all_analyses:
            return {'status': 'no_data', 'analyses': []}

        first_date = min(target_dates)
        last_date = max(target_dates)

        # çš„ä¸­/å¤–ã‚Œã§åˆ†é¡
        hits = [a for a in all_analyses if a['is_hit']]
        places = [a for a in all_analyses if a['is_place'] and not a['is_hit']]
        misses = [a for a in all_analyses if not a['is_place']]

        # ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’é›†è¨ˆ
        hit_contributions = self._aggregate_contributions(hits)
        miss_contributions = self._aggregate_contributions(misses)

        # å·®åˆ†ã‚’è¨ˆç®—ï¼ˆçš„ä¸­æ™‚ - å¤–ã‚Œæ™‚ï¼‰
        diff_contributions = {}
        all_features = set(hit_contributions.keys()) | set(miss_contributions.keys())
        for fname in all_features:
            hit_val = hit_contributions.get(fname, 0)
            miss_val = miss_contributions.get(fname, 0)
            diff_contributions[fname] = hit_val - miss_val

        # é‡è¦ãªå·®åˆ†ã‚’ã‚½ãƒ¼ãƒˆ
        sorted_diff = sorted(diff_contributions.items(), key=lambda x: abs(x[1]), reverse=True)

        return {
            'status': 'success',
            'period': f"{first_date} - {last_date}",
            'total_races': len(all_analyses),
            'hit_count': len(hits),
            'place_count': len(places),
            'miss_count': len(misses),
            'hit_rate': len(hits) / len(all_analyses) * 100 if all_analyses else 0,
            'place_rate': (len(hits) + len(places)) / len(all_analyses) * 100 if all_analyses else 0,
            'hit_contributions': hit_contributions,
            'miss_contributions': miss_contributions,
            'diff_contributions': dict(sorted_diff[:20]),  # ä¸Šä½20ä»¶
            'analyses': all_analyses,
        }

    def analyze_weekend(self, saturday: date, sunday: date) -> Dict:
        """é€±æœ«ã®ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
        return self.analyze_dates([saturday, sunday])

    def calculate_feature_adjustments(self, analysis: Dict, threshold: float = 0.1) -> Dict[str, float]:
        """
        SHAPåˆ†æçµæœã‹ã‚‰ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’è¨ˆç®—

        Args:
            analysis: analyze_dates()ã®çµæœ
            threshold: èª¿æ•´å¯¾è±¡ã¨ã™ã‚‹å·®åˆ†ã®é–¾å€¤

        Returns:
            ç‰¹å¾´é‡å â†’ èª¿æ•´ä¿‚æ•°ï¼ˆ1.0ãŒåŸºæº–ã€<1.0ã¯æŠ‘åˆ¶ã€>1.0ã¯å¼·åŒ–ï¼‰
        """
        if analysis.get('status') != 'success':
            return {}

        diff = analysis.get('diff_contributions', {})
        hit_contrib = analysis.get('hit_contributions', {})
        miss_contrib = analysis.get('miss_contributions', {})

        adjustments = {}

        for fname in self.feature_names:
            diff_val = diff.get(fname, 0)

            # å·®åˆ†ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆã®ã¿èª¿æ•´
            if abs(diff_val) < threshold:
                adjustments[fname] = 1.0
                continue

            # å¤–ã‚Œæ™‚ã«é«˜ã„ï¼ˆdiff < 0ï¼‰â†’ æŠ‘åˆ¶ï¼ˆä¿‚æ•° < 1.0ï¼‰
            # çš„ä¸­æ™‚ã«é«˜ã„ï¼ˆdiff > 0ï¼‰â†’ å¼·åŒ–ï¼ˆä¿‚æ•° > 1.0ï¼‰
            # ã‚¹ã‚±ãƒ¼ãƒ«: diff_val * 0.5 ã§æœ€å¤§Â±50%èª¿æ•´
            adjustment = 1.0 + (diff_val * 0.5)

            # 0.5ã€œ1.5ã®ç¯„å›²ã«åˆ¶é™
            adjustment = max(0.5, min(1.5, adjustment))
            adjustments[fname] = round(adjustment, 4)

        return adjustments

    def save_adjustments_to_db(self, adjustments: Dict[str, float], analysis_date: date = None) -> bool:
        """ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã«ä¿å­˜"""
        if not adjustments:
            return False

        if analysis_date is None:
            analysis_date = date.today()

        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # feature_adjustmentsãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            cur.execute('''
                INSERT INTO feature_adjustments (
                    adjustment_date, adjustments, is_active, created_at
                ) VALUES (%s, %s, TRUE, CURRENT_TIMESTAMP)
                ON CONFLICT (adjustment_date) DO UPDATE SET
                    adjustments = EXCLUDED.adjustments,
                    is_active = TRUE,
                    created_at = CURRENT_TIMESTAMP
            ''', (analysis_date, json.dumps(adjustments)))

            # å¤ã„èª¿æ•´ä¿‚æ•°ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–ï¼ˆç›´è¿‘3ä»¶ã®ã¿ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼‰
            cur.execute('''
                UPDATE feature_adjustments
                SET is_active = FALSE
                WHERE adjustment_date NOT IN (
                    SELECT adjustment_date FROM feature_adjustments
                    ORDER BY adjustment_date DESC LIMIT 3
                )
            ''')

            conn.commit()
            logger.info(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã«ä¿å­˜: {len(adjustments)}ä»¶")
            return True

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                conn.close()

    @staticmethod
    def load_adjustments_from_db() -> Dict[str, float]:
        """æœ€æ–°ã®ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã‹ã‚‰èª­ã¿è¾¼ã¿"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # æœ€æ–°ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªèª¿æ•´ä¿‚æ•°ã‚’å–å¾—
            cur.execute('''
                SELECT adjustments FROM feature_adjustments
                WHERE is_active = TRUE
                ORDER BY adjustment_date DESC
                LIMIT 1
            ''')

            row = cur.fetchone()
            cur.close()

            if row and row[0]:
                adjustments = row[0]
                if isinstance(adjustments, str):
                    adjustments = json.loads(adjustments)
                logger.info(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’èª­ã¿è¾¼ã¿: {len(adjustments)}ä»¶")
                return adjustments

            return {}

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _aggregate_contributions(self, analyses: List[Dict]) -> Dict[str, float]:
        """ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’é›†è¨ˆ"""
        if not analyses:
            return {}

        aggregated = {}
        for analysis in analyses:
            for fname, value in analysis.get('feature_contributions', {}).items():
                if fname not in aggregated:
                    aggregated[fname] = []
                aggregated[fname].append(value)

        # å¹³å‡ã‚’è¨ˆç®—
        return {fname: np.mean(values) for fname, values in aggregated.items()}

    def generate_report(self, analysis: Dict) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if analysis.get('status') != 'success':
            return "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"

        lines = [
            "ğŸ“Š **SHAPç‰¹å¾´é‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ**",
            f"æœŸé–“: {analysis['period']}",
            f"åˆ†æãƒ¬ãƒ¼ã‚¹æ•°: {analysis['total_races']}R",
            f"å˜å‹çš„ä¸­: {analysis['hit_count']}R ({analysis['hit_rate']:.1f}%)",
            f"è¤‡å‹åœ: {analysis['hit_count'] + analysis['place_count']}R ({analysis['place_rate']:.1f}%)",
            "",
            "**ã€çš„ä¸­/å¤–ã‚Œã§å·®ãŒå¤§ãã„ç‰¹å¾´é‡ã€‘**",
            "(æ­£: çš„ä¸­æ™‚ã«é«˜ã„ã€è² : å¤–ã‚Œæ™‚ã«é«˜ã„)",
        ]

        diff = analysis.get('diff_contributions', {})
        for i, (fname, value) in enumerate(list(diff.items())[:10]):
            sign = "+" if value > 0 else ""
            lines.append(f"  {i+1}. {fname}: {sign}{value:.4f}")

        # å¤–ã‚Œãƒ¬ãƒ¼ã‚¹ã§éå¤§è©•ä¾¡ã—ãŸç‰¹å¾´é‡
        miss_contrib = analysis.get('miss_contributions', {})
        if miss_contrib:
            sorted_miss = sorted(miss_contrib.items(), key=lambda x: x[1], reverse=True)
            lines.append("")
            lines.append("**ã€å¤–ã‚Œæ™‚ã«éå¤§è©•ä¾¡ã—ãŸç‰¹å¾´é‡ã€‘**")
            for i, (fname, value) in enumerate(sorted_miss[:5]):
                lines.append(f"  {i+1}. {fname}: {value:.4f}")

        # çš„ä¸­ãƒ¬ãƒ¼ã‚¹ã§åŠ¹ã„ãŸç‰¹å¾´é‡
        hit_contrib = analysis.get('hit_contributions', {})
        if hit_contrib:
            sorted_hit = sorted(hit_contrib.items(), key=lambda x: x[1], reverse=True)
            lines.append("")
            lines.append("**ã€çš„ä¸­æ™‚ã«å¯„ä¸ã—ãŸç‰¹å¾´é‡ã€‘**")
            for i, (fname, value) in enumerate(sorted_hit[:5]):
                lines.append(f"  {i+1}. {fname}: {value:.4f}")

        return "\n".join(lines)

    def send_discord_notification(self, report: str):
        """Discordé€šçŸ¥ã‚’é€ä¿¡"""
        import os
        import requests

        bot_token = os.getenv('DISCORD_BOT_TOKEN')
        channel_id = os.getenv('DISCORD_NOTIFICATION_CHANNEL_ID')

        if not bot_token or not channel_id:
            logger.warning("Discordé€šçŸ¥è¨­å®šãŒã‚ã‚Šã¾ã›ã‚“")
            return

        url = f"https://discord.com/api/v10/channels/{channel_id}/messages"
        headers = {
            "Authorization": f"Bot {bot_token}",
            "Content-Type": "application/json"
        }

        try:
            # 2000æ–‡å­—åˆ¶é™å¯¾å¿œ
            if len(report) > 1900:
                report = report[:1900] + "\n..."

            response = requests.post(url, headers=headers, json={"content": report}, timeout=10)
            if response.status_code in (200, 201):
                logger.info("SHAPåˆ†æDiscordé€šçŸ¥é€ä¿¡å®Œäº†")
            else:
                logger.warning(f"Discordé€šçŸ¥å¤±æ•—: {response.status_code}")
        except Exception as e:
            logger.error(f"Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def save_analysis_to_db(self, analysis: Dict) -> bool:
        """åˆ†æçµæœã‚’DBã«ä¿å­˜"""
        if analysis.get('status') != 'success':
            return False

        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # shap_analysisãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            cur.execute('''
                INSERT INTO shap_analysis (
                    analysis_date, period_start, period_end,
                    total_races, hit_count, place_count, miss_count,
                    hit_rate, place_rate,
                    hit_contributions, miss_contributions, diff_contributions,
                    created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
                ON CONFLICT (analysis_date) DO UPDATE SET
                    total_races = EXCLUDED.total_races,
                    hit_count = EXCLUDED.hit_count,
                    place_count = EXCLUDED.place_count,
                    miss_count = EXCLUDED.miss_count,
                    hit_rate = EXCLUDED.hit_rate,
                    place_rate = EXCLUDED.place_rate,
                    hit_contributions = EXCLUDED.hit_contributions,
                    miss_contributions = EXCLUDED.miss_contributions,
                    diff_contributions = EXCLUDED.diff_contributions,
                    created_at = CURRENT_TIMESTAMP
            ''', (
                date.today(),
                analysis['period'].split(' - ')[0],
                analysis['period'].split(' - ')[1],
                analysis['total_races'],
                analysis['hit_count'],
                analysis['place_count'],
                analysis['miss_count'],
                analysis['hit_rate'],
                analysis['place_rate'],
                json.dumps(analysis.get('hit_contributions', {})),
                json.dumps(analysis.get('miss_contributions', {})),
                json.dumps(analysis.get('diff_contributions', {})),
            ))

            conn.commit()
            logger.info("SHAPåˆ†æçµæœã‚’DBã«ä¿å­˜")
            return True

        except Exception as e:
            logger.error(f"SHAPåˆ†æDBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                conn.close()


def analyze_last_weekend():
    """å…ˆé€±æœ«ã®åˆ†æã‚’å®Ÿè¡Œï¼ˆäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ—¥ã‚’è‡ªå‹•æ¤œå‡ºï¼‰"""
    if not SHAP_AVAILABLE:
        logger.error("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install shap")
        return

    analyzer = ShapAnalyzer()

    # äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ç›´è¿‘ã®æ—¥ã‚’å–å¾—ï¼ˆæœ€å¤§7æ—¥å‰ã¾ã§ï¼‰
    race_dates = analyzer.get_recent_race_dates(days_back=7)

    if not race_dates:
        print("ç›´è¿‘7æ—¥é–“ã«äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ç›´è¿‘2æ—¥åˆ†ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹
    target_dates = sorted(race_dates)[-2:] if len(race_dates) >= 2 else race_dates
    first_date = target_dates[0]
    last_date = target_dates[-1]

    print(f"\n=== SHAPç‰¹å¾´é‡åˆ†æ ({first_date} - {last_date}) ===\n")
    print(f"å¯¾è±¡æ—¥: {', '.join(str(d) for d in target_dates)}")

    analysis = analyzer.analyze_dates(target_dates)

    if analysis['status'] == 'success':
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = analyzer.generate_report(analysis)
        print(report)

        # Discordé€šçŸ¥
        analyzer.send_discord_notification(report)

        # DBä¿å­˜
        analyzer.save_analysis_to_db(analysis)
    else:
        print("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="SHAPç‰¹å¾´é‡åˆ†æ")
    parser.add_argument("--date", "-d", help="åˆ†ææ—¥ (YYYY-MM-DD)")
    parser.add_argument("--weekend", "-w", action="store_true", help="å…ˆé€±æœ«ã‚’åˆ†æ")

    args = parser.parse_args()

    if args.weekend or not args.date:
        analyze_last_weekend()
    else:
        # ç‰¹å®šæ—¥ã®åˆ†æ
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        print(f"\n=== SHAPç‰¹å¾´é‡åˆ†æ ({target_date}) ===\n")

        analyzer = ShapAnalyzer()
        predictions = analyzer.get_predictions_from_db(target_date)

        analyses = []
        for pred in predictions:
            race_code = pred['race_code']
            analysis = analyzer.analyze_race(race_code, pred)
            if analysis:
                analysis['date'] = str(target_date)
                analyses.append(analysis)

        if analyses:
            # ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆ
            hits = [a for a in analyses if a['is_hit']]
            places = [a for a in analyses if a['is_place']]
            print(f"åˆ†æãƒ¬ãƒ¼ã‚¹æ•°: {len(analyses)}")
            print(f"å˜å‹çš„ä¸­: {len(hits)}R ({len(hits)/len(analyses)*100:.1f}%)")
            print(f"è¤‡å‹åœ: {len(places)}R ({len(places)/len(analyses)*100:.1f}%)")
        else:
            print("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
