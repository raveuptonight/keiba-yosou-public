"""
SHAPåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

é€±æœ«ãƒ¬ãƒ¼ã‚¹ã®äºˆæƒ³çµæœã‚’åˆ†æã—ã€ç‰¹å¾´é‡ã®å¯„ä¸åº¦ã‚’å¯è¦–åŒ–ã™ã‚‹
- çš„ä¸­/å¤–ã‚Œåˆ¥ã®ç‰¹å¾´é‡é‡è¦åº¦æ¯”è¼ƒ
- éå¤§è©•ä¾¡/éå°è©•ä¾¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
- é€±æ¬¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import json
import logging
from datetime import date, datetime, timedelta

import joblib
import numpy as np
import pandas as pd

try:
    import shap

    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

from src.db.connection import get_db
from src.models.feature_extractor import FastFeatureExtractor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class ShapAnalyzer:
    """SHAPå€¤ã«ã‚ˆã‚‹äºˆæƒ³åˆ†æ"""

    def __init__(self, model_path: str = "/app/models/ensemble_model_latest.pkl"):
        self.model_path = model_path
        self.xgb_model = None
        self.lgb_model = None
        self.feature_names = None
        self.explainer = None
        self._load_model()

    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            model_data = joblib.load(self.model_path)
            models_dict = model_data.get("models", {})

            # å›å¸°ãƒ¢ãƒ‡ãƒ«å–å¾—
            if "xgb_regressor" in models_dict:
                self.xgb_model = models_dict["xgb_regressor"]
                self.lgb_model = models_dict.get("lgb_regressor")
            elif "xgb_model" in model_data:
                self.xgb_model = model_data["xgb_model"]
                self.lgb_model = model_data.get("lgb_model")
            elif "xgboost" in models_dict:
                self.xgb_model = models_dict.get("xgboost")
                self.lgb_model = models_dict.get("lightgbm")

            self.feature_names = model_data.get("feature_names", [])
            logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(self.feature_names)}ç‰¹å¾´é‡")

            # SHAP Explainerã‚’åˆæœŸåŒ–ï¼ˆXGBoostã‚’ä½¿ç”¨ï¼‰
            if SHAP_AVAILABLE and self.xgb_model is not None:
                self.explainer = shap.TreeExplainer(self.xgb_model)
                logger.info("SHAP TreeExplaineråˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

    def get_recent_race_dates(self, days_back: int = 7) -> list[date]:
        """ç›´è¿‘ã®ãƒ¬ãƒ¼ã‚¹æ—¥ï¼ˆäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ—¥ï¼‰ã‚’å–å¾—"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            cur.execute(
                """
                SELECT DISTINCT race_date
                FROM predictions
                WHERE race_date >= %s AND race_date < %s
                ORDER BY race_date DESC
            """,
                (date.today() - timedelta(days=days_back), date.today()),
            )

            dates = [row[0] for row in cur.fetchall()]
            cur.close()
            return dates
        finally:
            conn.close()

    def get_predictions_from_db(self, target_date: date) -> list[dict]:
        """DBã‹ã‚‰äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ã‚¹ã”ã¨ã«æœ€æ–°ã®äºˆæƒ³ã®ã¿ï¼‰"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            cur.execute(
                """
                SELECT DISTINCT ON (race_id)
                    prediction_id,
                    race_id,
                    race_date,
                    prediction_result,
                    predicted_at
                FROM predictions
                WHERE race_date = %s
                ORDER BY race_id, predicted_at DESC
            """,
                (target_date,),
            )

            predictions = []
            for row in cur.fetchall():
                prediction_result = row[3]
                if isinstance(prediction_result, str):
                    try:
                        prediction_result = json.loads(prediction_result)
                    except json.JSONDecodeError:
                        prediction_result = {}

                predictions.append(
                    {
                        "prediction_id": row[0],
                        "race_code": row[1],
                        "race_date": row[2],
                        "prediction_result": prediction_result,
                    }
                )

            cur.close()
            logger.info(f"äºˆæƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(predictions)}ãƒ¬ãƒ¼ã‚¹ ({target_date})")
            return predictions

        finally:
            conn.close()

    def get_race_results(self, target_date: date) -> dict[str, dict]:
        """ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()
            kaisai_gappi = target_date.strftime("%m%d")
            kaisai_nen = str(target_date.year)

            cur.execute(
                """
                SELECT race_code, umaban, kakutei_chakujun, bamei
                FROM umagoto_race_joho
                WHERE race_code IN (
                    SELECT DISTINCT race_code
                    FROM race_shosai
                    WHERE kaisai_nen = %s AND kaisai_gappi = %s
                      AND data_kubun IN ('6', '7')
                )
                AND data_kubun IN ('6', '7')
                ORDER BY race_code, kakutei_chakujun::int
            """,
                (kaisai_nen, kaisai_gappi),
            )

            results = {}
            for row in cur.fetchall():
                race_code = row[0]
                if race_code not in results:
                    results[race_code] = {"horses": []}
                results[race_code]["horses"].append(
                    {
                        "umaban": row[1],
                        "chakujun": int(row[2]) if row[2] else 99,
                        "bamei": row[3],
                    }
                )

            cur.close()
            return results

        finally:
            conn.close()

    def extract_features_for_race(self, race_code: str) -> pd.DataFrame | None:
        """ãƒ¬ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        db = get_db()
        conn = db.get_connection()

        try:
            extractor = FastFeatureExtractor(conn)
            cur = conn.cursor()

            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—
            cur.execute(
                """
                SELECT kaisai_nen, race_code, kaisai_gappi, keibajo_code,
                       kyori, track_code, grade_code,
                       shiba_babajotai_code, dirt_babajotai_code
                FROM race_shosai
                WHERE race_code = %s
            """,
                (race_code,),
            )
            race_row = cur.fetchone()
            if not race_row:
                return None

            race_cols = [
                "kaisai_nen",
                "race_code",
                "kaisai_gappi",
                "keibajo_code",
                "kyori",
                "track_code",
                "grade_code",
                "shiba_babajotai_code",
                "dirt_babajotai_code",
            ]
            races = [dict(zip(race_cols, race_row))]
            year = int(race_row[0])

            # å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾—
            cur.execute(
                """
                SELECT
                    race_code, umaban, wakuban, ketto_toroku_bango,
                    seibetsu_code, barei, futan_juryo,
                    blinker_shiyo_kubun, kishu_code, chokyoshi_code,
                    bataiju, zogen_sa, bamei, kakutei_chakujun
                FROM umagoto_race_joho
                WHERE race_code = %s
                  AND data_kubun IN ('6', '7')
                ORDER BY umaban::int
            """,
                (race_code,),
            )

            cols = [d[0] for d in cur.description]
            rows = cur.fetchall()
            entries = [dict(zip(cols, row)) for row in rows]

            if not entries:
                return None

            # éå»æˆç¸¾å–å¾—
            kettonums = [e["ketto_toroku_bango"] for e in entries if e.get("ketto_toroku_bango")]
            past_stats = extractor._get_past_stats_batch(kettonums)

            # é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            extractor._cache_jockey_trainer_stats(year)

            # è¿½åŠ ãƒ‡ãƒ¼ã‚¿
            jh_pairs = [
                (e.get("kishu_code", ""), e.get("ketto_toroku_bango", ""))
                for e in entries
                if e.get("kishu_code") and e.get("ketto_toroku_bango")
            ]
            jockey_horse_stats = extractor._get_jockey_horse_combo_batch(jh_pairs)
            surface_stats = extractor._get_surface_stats_batch(kettonums)
            turn_stats = extractor._get_turn_rates_batch(kettonums)
            for kettonum, stats in turn_stats.items():
                if kettonum in past_stats:
                    past_stats[kettonum]["right_turn_rate"] = stats["right_turn_rate"]
                    past_stats[kettonum]["left_turn_rate"] = stats["left_turn_rate"]
            training_stats = extractor._get_training_stats_batch(kettonums)

            # ç‰¹å¾´é‡ç”Ÿæˆ
            features_list = []
            for entry in entries:
                features = extractor._build_features(
                    entry,
                    races,
                    past_stats,
                    jockey_horse_stats=jockey_horse_stats,
                    distance_stats=surface_stats,
                    training_stats=training_stats,
                )
                if features:
                    features["bamei"] = entry.get("bamei", "")
                    features["actual_chakujun"] = int(entry.get("kakutei_chakujun", 99))
                    features_list.append(features)

            cur.close()

            if not features_list:
                return None

            return pd.DataFrame(features_list)

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({race_code}): {e}")
            return None
        finally:
            conn.close()

    def calculate_shap_values(self, X: pd.DataFrame) -> np.ndarray:
        """SHAPå€¤ã‚’è¨ˆç®—"""
        if not SHAP_AVAILABLE:
            logger.warning("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None

        if self.explainer is None:
            logger.warning("SHAP ExplainerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        try:
            shap_values = self.explainer.shap_values(X)
            return shap_values
        except Exception as e:
            logger.error(f"SHAPå€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def analyze_race(self, race_code: str, prediction: dict) -> dict | None:
        """1ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æï¼ˆEVæ¨å¥¨ãƒ»è»¸é¦¬å½¢å¼ï¼‰"""
        # ç‰¹å¾´é‡æŠ½å‡º
        df = self.extract_features_for_race(race_code)
        if df is None or df.empty:
            return None

        # äºˆæƒ³çµæœã‹ã‚‰é¦¬ã‚’å–å¾—
        ranked_horses = prediction.get("prediction_result", {}).get("ranked_horses", [])
        if not ranked_horses:
            return None

        # EVæ¨å¥¨é¦¬ã‚’ç‰¹å®šï¼ˆEV >= 1.5ï¼‰
        ev_recommended = []
        for h in ranked_horses:
            win_prob = h.get("win_probability", 0)
            odds = h.get("predicted_odds", 0)
            if odds > 0 and win_prob > 0:
                win_ev = win_prob * odds
                if win_ev >= 1.5:
                    ev_recommended.append(h)

        # è»¸é¦¬ã‚’ç‰¹å®šï¼ˆè¤‡å‹ç¢ºç‡æœ€é«˜ï¼‰
        axis_horse = (
            max(ranked_horses, key=lambda h: h.get("place_probability", 0))
            if ranked_horses
            else None
        )

        # ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
        X = df[self.feature_names].fillna(0)

        # SHAPå€¤è¨ˆç®—
        shap_values = self.calculate_shap_values(X)
        if shap_values is None:
            return None

        # è»¸é¦¬ã®åˆ†æ
        axis_umaban = str(axis_horse.get("horse_number", "")).zfill(2) if axis_horse else None
        axis_analysis = None
        if axis_umaban:
            horse_row = df[df["umaban"].astype(str).str.zfill(2) == axis_umaban]
            if not horse_row.empty:
                actual_chakujun = int(horse_row["actual_chakujun"].iloc[0])
                horse_idx = horse_row.index[0]
                horse_shap = shap_values[horse_idx]
                axis_contributions = {
                    fname: float(horse_shap[i]) for i, fname in enumerate(self.feature_names)
                }
                axis_analysis = {
                    "umaban": axis_umaban,
                    "bamei": axis_horse.get("horse_name", ""),
                    "actual_chakujun": actual_chakujun,
                    "is_place": actual_chakujun <= 3,
                    "place_prob": axis_horse.get("place_probability", 0),
                    "feature_contributions": axis_contributions,
                }

        # EVæ¨å¥¨é¦¬ã®åˆ†æ
        ev_analyses = []
        for h in ev_recommended:
            h_umaban = str(h.get("horse_number", "")).zfill(2)
            horse_row = df[df["umaban"].astype(str).str.zfill(2) == h_umaban]
            if not horse_row.empty:
                actual_chakujun = int(horse_row["actual_chakujun"].iloc[0])
                horse_idx = horse_row.index[0]
                horse_shap = shap_values[horse_idx]
                ev_contributions = {
                    fname: float(horse_shap[i]) for i, fname in enumerate(self.feature_names)
                }
                win_prob = h.get("win_probability", 0)
                odds = h.get("predicted_odds", 0)
                ev_analyses.append(
                    {
                        "umaban": h_umaban,
                        "bamei": h.get("horse_name", ""),
                        "actual_chakujun": actual_chakujun,
                        "is_hit": actual_chakujun == 1,
                        "is_place": actual_chakujun <= 3,
                        "win_ev": win_prob * odds,
                        "win_prob": win_prob,
                        "feature_contributions": ev_contributions,
                    }
                )

        return {
            "race_code": race_code,
            "axis_analysis": axis_analysis,
            "ev_analyses": ev_analyses,
            "has_ev_rec": len(ev_recommended) > 0,
        }

    def analyze_dates(self, target_dates: list[date]) -> dict:
        """è¤‡æ•°æ—¥ã®ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æï¼ˆEVæ¨å¥¨ãƒ»è»¸é¦¬å½¢å¼ï¼‰"""
        all_analyses = []

        for target_date in target_dates:
            predictions = self.get_predictions_from_db(target_date)

            for pred in predictions:
                race_code = pred["race_code"]
                analysis = self.analyze_race(race_code, pred)
                if analysis:
                    analysis["date"] = str(target_date)
                    all_analyses.append(analysis)

        if not all_analyses:
            return {"status": "no_data", "analyses": []}

        first_date = min(target_dates)
        last_date = max(target_dates)

        # è»¸é¦¬ã®çš„ä¸­/å¤–ã‚Œã§åˆ†é¡
        axis_places = [
            a for a in all_analyses if a.get("axis_analysis") and a["axis_analysis"].get("is_place")
        ]
        axis_misses = [
            a
            for a in all_analyses
            if a.get("axis_analysis") and not a["axis_analysis"].get("is_place")
        ]

        # EVæ¨å¥¨é¦¬ã®çš„ä¸­/å¤–ã‚Œã§åˆ†é¡
        ev_hits = []
        ev_misses = []
        for a in all_analyses:
            for ev in a.get("ev_analyses", []):
                if ev.get("is_hit"):
                    ev_hits.append({"feature_contributions": ev["feature_contributions"]})
                elif not ev.get("is_place"):
                    ev_misses.append({"feature_contributions": ev["feature_contributions"]})

        # è»¸é¦¬ã®ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’é›†è¨ˆ
        axis_place_contributions = self._aggregate_contributions(
            [
                {"feature_contributions": a["axis_analysis"]["feature_contributions"]}
                for a in axis_places
            ]
        )
        axis_miss_contributions = self._aggregate_contributions(
            [
                {"feature_contributions": a["axis_analysis"]["feature_contributions"]}
                for a in axis_misses
            ]
        )

        # EVæ¨å¥¨é¦¬ã®ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’é›†è¨ˆ
        ev_hit_contributions = self._aggregate_contributions(ev_hits)
        ev_miss_contributions = self._aggregate_contributions(ev_misses)

        # å·®åˆ†ã‚’è¨ˆç®—ï¼ˆçš„ä¸­æ™‚ - å¤–ã‚Œæ™‚ï¼‰
        axis_diff = {}
        all_features = set(axis_place_contributions.keys()) | set(axis_miss_contributions.keys())
        for fname in all_features:
            place_val = axis_place_contributions.get(fname, 0)
            miss_val = axis_miss_contributions.get(fname, 0)
            axis_diff[fname] = place_val - miss_val

        ev_diff = {}
        all_features = set(ev_hit_contributions.keys()) | set(ev_miss_contributions.keys())
        for fname in all_features:
            hit_val = ev_hit_contributions.get(fname, 0)
            miss_val = ev_miss_contributions.get(fname, 0)
            ev_diff[fname] = hit_val - miss_val

        # é‡è¦ãªå·®åˆ†ã‚’ã‚½ãƒ¼ãƒˆ
        sorted_axis_diff = sorted(axis_diff.items(), key=lambda x: abs(x[1]), reverse=True)
        sorted_ev_diff = sorted(ev_diff.items(), key=lambda x: abs(x[1]), reverse=True)

        # EVæ¨å¥¨é¦¬ã®æˆç¸¾é›†è¨ˆ
        total_ev_count = sum(len(a.get("ev_analyses", [])) for a in all_analyses)
        ev_tansho_hits = sum(
            1 for a in all_analyses for ev in a.get("ev_analyses", []) if ev.get("is_hit")
        )
        ev_fukusho_hits = sum(
            1 for a in all_analyses for ev in a.get("ev_analyses", []) if ev.get("is_place")
        )

        return {
            "status": "success",
            "period": f"{first_date} - {last_date}",
            "total_races": len(all_analyses),
            # è»¸é¦¬æˆç¸¾
            "axis_place_count": len(axis_places),
            "axis_miss_count": len(axis_misses),
            "axis_place_rate": len(axis_places) / len(all_analyses) * 100 if all_analyses else 0,
            "axis_place_contributions": axis_place_contributions,
            "axis_miss_contributions": axis_miss_contributions,
            "axis_diff_contributions": dict(sorted_axis_diff[:20]),
            # EVæ¨å¥¨æˆç¸¾
            "ev_rec_count": total_ev_count,
            "ev_tansho_hits": ev_tansho_hits,
            "ev_fukusho_hits": ev_fukusho_hits,
            "ev_tansho_rate": ev_tansho_hits / total_ev_count * 100 if total_ev_count > 0 else 0,
            "ev_fukusho_rate": ev_fukusho_hits / total_ev_count * 100 if total_ev_count > 0 else 0,
            "ev_hit_contributions": ev_hit_contributions,
            "ev_miss_contributions": ev_miss_contributions,
            "ev_diff_contributions": dict(sorted_ev_diff[:20]),
            "analyses": all_analyses,
        }

    def analyze_weekend(self, saturday: date, sunday: date) -> dict:
        """é€±æœ«ã®ãƒ¬ãƒ¼ã‚¹ã‚’åˆ†æï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
        return self.analyze_dates([saturday, sunday])

    def calculate_feature_adjustments(
        self, analysis: dict, threshold: float = 0.1
    ) -> dict[str, float]:
        """
        SHAPåˆ†æçµæœã‹ã‚‰ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’è¨ˆç®—

        Args:
            analysis: analyze_dates()ã®çµæœ
            threshold: èª¿æ•´å¯¾è±¡ã¨ã™ã‚‹å·®åˆ†ã®é–¾å€¤

        Returns:
            ç‰¹å¾´é‡å â†’ èª¿æ•´ä¿‚æ•°ï¼ˆ1.0ãŒåŸºæº–ã€<1.0ã¯æŠ‘åˆ¶ã€>1.0ã¯å¼·åŒ–ï¼‰
        """
        if analysis.get("status") != "success":
            return {}

        diff = analysis.get("diff_contributions", {})
        analysis.get("hit_contributions", {})
        analysis.get("miss_contributions", {})

        adjustments = {}

        for fname in self.feature_names:
            diff_val = diff.get(fname, 0)

            # å·®åˆ†ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆã®ã¿èª¿æ•´
            if abs(diff_val) < threshold:
                adjustments[fname] = 1.0
                continue

            # å¤–ã‚Œæ™‚ã«é«˜ã„ï¼ˆdiff < 0ï¼‰â†’ æŠ‘åˆ¶ï¼ˆä¿‚æ•° < 1.0ï¼‰
            # çš„ä¸­æ™‚ã«é«˜ã„ï¼ˆdiff > 0ï¼‰â†’ å¼·åŒ–ï¼ˆä¿‚æ•° > 1.0ï¼‰
            # ã‚¹ã‚±ãƒ¼ãƒ«: diff_val * 0.5 ã§æœ€å¤§Â±50%èª¿æ•´
            adjustment = 1.0 + (diff_val * 0.5)

            # 0.5ã€œ1.5ã®ç¯„å›²ã«åˆ¶é™
            adjustment = max(0.5, min(1.5, adjustment))
            adjustments[fname] = round(adjustment, 4)

        return adjustments

    def save_adjustments_to_db(
        self, adjustments: dict[str, float], analysis_date: date = None
    ) -> bool:
        """ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã«ä¿å­˜"""
        if not adjustments:
            return False

        if analysis_date is None:
            analysis_date = date.today()

        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # feature_adjustmentsãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            cur.execute(
                """
                INSERT INTO feature_adjustments (
                    adjustment_date, adjustments, is_active, created_at
                ) VALUES (%s, %s, TRUE, CURRENT_TIMESTAMP)
                ON CONFLICT (adjustment_date) DO UPDATE SET
                    adjustments = EXCLUDED.adjustments,
                    is_active = TRUE,
                    created_at = CURRENT_TIMESTAMP
            """,
                (analysis_date, json.dumps(adjustments)),
            )

            # å¤ã„èª¿æ•´ä¿‚æ•°ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–ï¼ˆç›´è¿‘3ä»¶ã®ã¿ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼‰
            cur.execute(
                """
                UPDATE feature_adjustments
                SET is_active = FALSE
                WHERE adjustment_date NOT IN (
                    SELECT adjustment_date FROM feature_adjustments
                    ORDER BY adjustment_date DESC LIMIT 3
                )
            """
            )

            conn.commit()
            logger.info(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã«ä¿å­˜: {len(adjustments)}ä»¶")
            return True

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                conn.close()

    @staticmethod
    def load_adjustments_from_db() -> dict[str, float]:
        """æœ€æ–°ã®ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’DBã‹ã‚‰èª­ã¿è¾¼ã¿"""
        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # æœ€æ–°ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªèª¿æ•´ä¿‚æ•°ã‚’å–å¾—
            cur.execute(
                """
                SELECT adjustments FROM feature_adjustments
                WHERE is_active = TRUE
                ORDER BY adjustment_date DESC
                LIMIT 1
            """
            )

            row = cur.fetchone()
            cur.close()

            if row and row[0]:
                adjustments = row[0]
                if isinstance(adjustments, str):
                    adjustments = json.loads(adjustments)
                logger.info(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°ã‚’èª­ã¿è¾¼ã¿: {len(adjustments)}ä»¶")
                return adjustments

            return {}

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡èª¿æ•´ä¿‚æ•°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
        finally:
            if conn:
                conn.close()

    def _aggregate_contributions(self, analyses: list[dict]) -> dict[str, float]:
        """ç‰¹å¾´é‡å¯„ä¸åº¦ã‚’é›†è¨ˆ"""
        if not analyses:
            return {}

        aggregated = {}
        for analysis in analyses:
            for fname, value in analysis.get("feature_contributions", {}).items():
                if fname not in aggregated:
                    aggregated[fname] = []
                aggregated[fname].append(value)

        # å¹³å‡ã‚’è¨ˆç®—
        return {fname: np.mean(values) for fname, values in aggregated.items()}

    def generate_report(self, analysis: dict) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆEVæ¨å¥¨ãƒ»è»¸é¦¬å½¢å¼ï¼‰"""
        if analysis.get("status") != "success":
            return "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"

        lines = [
            "ğŸ“Š **SHAPç‰¹å¾´é‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ**",
            f"æœŸé–“: {analysis['period']}",
            f"åˆ†æãƒ¬ãƒ¼ã‚¹æ•°: {analysis['total_races']}R",
            "",
        ]

        # è»¸é¦¬æˆç¸¾
        lines.append("**ã€è»¸é¦¬æˆç¸¾ã€‘** (è¤‡å‹ç¢ºç‡1ä½)")
        axis_place = analysis.get("axis_place_count", 0)
        axis_total = axis_place + analysis.get("axis_miss_count", 0)
        lines.append(
            f"  è¤‡å‹çš„ä¸­: {axis_place}/{axis_total}R ({analysis.get('axis_place_rate', 0):.1f}%)"
        )

        # è»¸é¦¬ã®ç‰¹å¾´é‡å·®åˆ†
        axis_diff = analysis.get("axis_diff_contributions", {})
        if axis_diff:
            lines.append("")
            lines.append("**ã€è»¸é¦¬: è¤‡å‹çš„ä¸­/å¤–ã‚Œã§å·®ãŒå¤§ãã„ç‰¹å¾´é‡ã€‘**")
            lines.append("(æ­£: çš„ä¸­æ™‚ã«é«˜ã„ã€è² : å¤–ã‚Œæ™‚ã«é«˜ã„)")
            for i, (fname, value) in enumerate(list(axis_diff.items())[:7]):
                sign = "+" if value > 0 else ""
                lines.append(f"  {i+1}. {fname}: {sign}{value:.4f}")

        # EVæ¨å¥¨æˆç¸¾
        lines.append("")
        ev_count = analysis.get("ev_rec_count", 0)
        if ev_count > 0:
            lines.append("**ã€EVæ¨å¥¨æˆç¸¾ã€‘** (EV >= 1.5)")
            lines.append(f"  æ¨å¥¨é¦¬æ•°: {ev_count}é ­")
            lines.append(
                f"  å˜å‹çš„ä¸­: {analysis.get('ev_tansho_hits', 0)} ({analysis.get('ev_tansho_rate', 0):.1f}%)"
            )
            lines.append(
                f"  è¤‡å‹çš„ä¸­: {analysis.get('ev_fukusho_hits', 0)} ({analysis.get('ev_fukusho_rate', 0):.1f}%)"
            )

            # EVæ¨å¥¨ã®ç‰¹å¾´é‡å·®åˆ†
            ev_diff = analysis.get("ev_diff_contributions", {})
            if ev_diff:
                lines.append("")
                lines.append("**ã€EVæ¨å¥¨: çš„ä¸­/å¤–ã‚Œã§å·®ãŒå¤§ãã„ç‰¹å¾´é‡ã€‘**")
                for i, (fname, value) in enumerate(list(ev_diff.items())[:7]):
                    sign = "+" if value > 0 else ""
                    lines.append(f"  {i+1}. {fname}: {sign}{value:.4f}")
        else:
            lines.append("**ã€EVæ¨å¥¨æˆç¸¾ã€‘**")
            lines.append("  EVæ¨å¥¨ãªã—")

        return "\n".join(lines)

    def send_discord_notification(self, report: str):
        """Discordé€šçŸ¥ã‚’é€ä¿¡"""
        import os

        import requests

        bot_token = os.getenv("DISCORD_BOT_TOKEN")
        channel_id = os.getenv("DISCORD_NOTIFICATION_CHANNEL_ID")

        if not bot_token or not channel_id:
            logger.warning("Discordé€šçŸ¥è¨­å®šãŒã‚ã‚Šã¾ã›ã‚“")
            return

        url = f"https://discord.com/api/v10/channels/{channel_id}/messages"
        headers = {"Authorization": f"Bot {bot_token}", "Content-Type": "application/json"}

        try:
            # 2000æ–‡å­—åˆ¶é™å¯¾å¿œ
            if len(report) > 1900:
                report = report[:1900] + "\n..."

            response = requests.post(url, headers=headers, json={"content": report}, timeout=10)
            if response.status_code in (200, 201):
                logger.info("SHAPåˆ†æDiscordé€šçŸ¥é€ä¿¡å®Œäº†")
            else:
                logger.warning(f"Discordé€šçŸ¥å¤±æ•—: {response.status_code}")
        except Exception as e:
            logger.error(f"Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def save_analysis_to_db(self, analysis: dict) -> bool:
        """åˆ†æçµæœã‚’DBã«ä¿å­˜"""
        if analysis.get("status") != "success":
            return False

        db = get_db()
        conn = db.get_connection()

        try:
            cur = conn.cursor()

            # shap_analysisãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
            cur.execute(
                """
                INSERT INTO shap_analysis (
                    analysis_date, period_start, period_end,
                    total_races, hit_count, place_count, miss_count,
                    hit_rate, place_rate,
                    hit_contributions, miss_contributions, diff_contributions,
                    created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
                ON CONFLICT (analysis_date) DO UPDATE SET
                    total_races = EXCLUDED.total_races,
                    hit_count = EXCLUDED.hit_count,
                    place_count = EXCLUDED.place_count,
                    miss_count = EXCLUDED.miss_count,
                    hit_rate = EXCLUDED.hit_rate,
                    place_rate = EXCLUDED.place_rate,
                    hit_contributions = EXCLUDED.hit_contributions,
                    miss_contributions = EXCLUDED.miss_contributions,
                    diff_contributions = EXCLUDED.diff_contributions,
                    created_at = CURRENT_TIMESTAMP
            """,
                (
                    date.today(),
                    analysis["period"].split(" - ")[0],
                    analysis["period"].split(" - ")[1],
                    analysis["total_races"],
                    analysis["hit_count"],
                    analysis["place_count"],
                    analysis["miss_count"],
                    analysis["hit_rate"],
                    analysis["place_rate"],
                    json.dumps(analysis.get("hit_contributions", {})),
                    json.dumps(analysis.get("miss_contributions", {})),
                    json.dumps(analysis.get("diff_contributions", {})),
                ),
            )

            conn.commit()
            logger.info("SHAPåˆ†æçµæœã‚’DBã«ä¿å­˜")
            return True

        except Exception as e:
            logger.error(f"SHAPåˆ†æDBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                conn.close()


def analyze_last_weekend():
    """å…ˆé€±æœ«ã®åˆ†æã‚’å®Ÿè¡Œï¼ˆäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ—¥ã‚’è‡ªå‹•æ¤œå‡ºï¼‰"""
    if not SHAP_AVAILABLE:
        logger.error("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install shap")
        return

    analyzer = ShapAnalyzer()

    # äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ç›´è¿‘ã®æ—¥ã‚’å–å¾—ï¼ˆæœ€å¤§7æ—¥å‰ã¾ã§ï¼‰
    race_dates = analyzer.get_recent_race_dates(days_back=7)

    if not race_dates:
        print("ç›´è¿‘7æ—¥é–“ã«äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ç›´è¿‘2æ—¥åˆ†ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹
    target_dates = sorted(race_dates)[-2:] if len(race_dates) >= 2 else race_dates
    first_date = target_dates[0]
    last_date = target_dates[-1]

    print(f"\n=== SHAPç‰¹å¾´é‡åˆ†æ ({first_date} - {last_date}) ===\n")
    print(f"å¯¾è±¡æ—¥: {', '.join(str(d) for d in target_dates)}")

    analysis = analyzer.analyze_dates(target_dates)

    if analysis["status"] == "success":
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = analyzer.generate_report(analysis)
        print(report)

        # Discordé€šçŸ¥
        analyzer.send_discord_notification(report)

        # DBä¿å­˜
        analyzer.save_analysis_to_db(analysis)
    else:
        print("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="SHAPç‰¹å¾´é‡åˆ†æ")
    parser.add_argument("--date", "-d", help="åˆ†ææ—¥ (YYYY-MM-DD)")
    parser.add_argument("--weekend", "-w", action="store_true", help="å…ˆé€±æœ«ã‚’åˆ†æ")

    args = parser.parse_args()

    if args.weekend or not args.date:
        analyze_last_weekend()
    else:
        # ç‰¹å®šæ—¥ã®åˆ†æ
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        print(f"\n=== SHAPç‰¹å¾´é‡åˆ†æ ({target_date}) ===\n")

        analyzer = ShapAnalyzer()
        predictions = analyzer.get_predictions_from_db(target_date)

        analyses = []
        for pred in predictions:
            race_code = pred["race_code"]
            analysis = analyzer.analyze_race(race_code, pred)
            if analysis:
                analysis["date"] = str(target_date)
                analyses.append(analysis)

        if analyses:
            # ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆ
            hits = [a for a in analyses if a["is_hit"]]
            places = [a for a in analyses if a["is_place"]]
            print(f"åˆ†æãƒ¬ãƒ¼ã‚¹æ•°: {len(analyses)}")
            print(f"å˜å‹çš„ä¸­: {len(hits)}R ({len(hits)/len(analyses)*100:.1f}%)")
            print(f"è¤‡å‹åœ: {len(places)}R ({len(places)/len(analyses)*100:.1f}%)")
        else:
            print("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
